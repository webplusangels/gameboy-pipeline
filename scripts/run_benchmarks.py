#!/usr/bin/env python3
"""
ë¦¬íŒ©í† ë§ ì „í›„ ì½”ë“œ í’ˆì§ˆ ë²¤ì¹˜ë§ˆí¬ ì¸¡ì • ë° ë¦¬í¬íŠ¸ ìƒì„±.

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€, ì½”ë“œ ë³µì¡ë„, ë¦°íŠ¸ ì´ìŠˆ ë“±ì„ ì¸¡ì •í•˜ê³ 
JSON(ê¸°ê³„ìš©) + Markdown(ì‚¬ëŒìš©) í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

Usage:
    uv run scripts/run_benchmarks.py baseline          # ë¦¬íŒ©í† ë§ ì „ ê¸°ì¤€ì„  ìº¡ì²˜
    uv run scripts/run_benchmarks.py snapshot phase1   # Phaseë³„ ìŠ¤ëƒ…ìƒ·
    uv run scripts/run_benchmarks.py compare baseline phase1  # ë¹„êµ ë¦¬í¬íŠ¸
    uv run scripts/run_benchmarks.py --help            # ë„ì›€ë§
"""

from __future__ import annotations

import argparse
import json
import re
import subprocess
import sys
from dataclasses import asdict, dataclass, field
from datetime import datetime
from pathlib import Path

# =============================================================================
# ê²½ë¡œ ì„¤ì •
# =============================================================================

SCRIPT_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = SCRIPT_DIR.parent
BENCHMARKS_DIR = PROJECT_ROOT / "docs" / "refactoring" / "benchmarks"
DATA_DIR = BENCHMARKS_DIR / "data"
REPORTS_DIR = BENCHMARKS_DIR / "reports"

SRC_DIR = PROJECT_ROOT / "src"
SCRIPTS_DIR = PROJECT_ROOT / "scripts"
TESTS_DIR = PROJECT_ROOT / "tests"


# =============================================================================
# ë°ì´í„° ëª¨ë¸
# =============================================================================


@dataclass
class FileComplexity:
    """íŒŒì¼ë³„ ë³µì¡ë„ ì •ë³´."""

    file_path: str
    line_count: int
    average_cc: float
    max_cc: float
    max_cc_function: str


@dataclass
class BenchmarkMetrics:
    """ë²¤ì¹˜ë§ˆí¬ ì¸¡ì • ê²°ê³¼."""

    timestamp: str
    tag: str

    # í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ (src/ë§Œ ì¸¡ì •, scripts/ ì œì™¸)
    test_count: int
    coverage_src: float

    # ì½”ë“œ í’ˆì§ˆ
    mypy_errors: int
    ruff_issues: int

    # ë³µì¡ë„
    complexity: list[FileComplexity] = field(default_factory=list)

    # ëª¨ë“ˆ êµ¬ì¡°
    src_file_count: int = 0
    avg_file_lines: float = 0.0
    total_src_lines: int = 0

    # ë¦¬íŒ©í† ë§ ì§€í‘œ
    max_function_lines: int = 0
    functions_over_50_lines: int = 0
    max_function_cc: int = 0
    maintainability_index: float = 0.0

    # ì‹¤í–‰ ì‹œê°„
    test_duration_seconds: float = 0.0


# =============================================================================
# ì¸¡ì • í•¨ìˆ˜ë“¤
# =============================================================================


def run_command(cmd: list[str], cwd: Path | None = None) -> tuple[int, str, str]:
    """ëª…ë ¹ì–´ ì‹¤í–‰ í›„ ê²°ê³¼ ë°˜í™˜.

    Args:
        cmd: ì‹¤í–‰í•  ëª…ë ¹ì–´ ë¦¬ìŠ¤íŠ¸.
        cwd: ì‘ì—… ë””ë ‰í† ë¦¬.

    Returns:
        (return_code, stdout, stderr) íŠœí”Œ.
    """
    result = subprocess.run(
        cmd,
        cwd=cwd or PROJECT_ROOT,
        capture_output=True,
        text=True,
        encoding="utf-8",
        errors="replace",  # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë¬¸ì ì‚¬ìš©
    )
    return result.returncode, result.stdout, result.stderr


def get_test_metrics() -> tuple[int, float, float]:
    """pytest ì‹¤í–‰í•˜ì—¬ í…ŒìŠ¤íŠ¸ ìˆ˜, ì»¤ë²„ë¦¬ì§€, ì‹¤í–‰ ì‹œê°„ ì¸¡ì •.

    Note:
        scripts/ëŠ” ì§„ì…ì /ê¸€ë£¨ ì½”ë“œì´ë¯€ë¡œ ì»¤ë²„ë¦¬ì§€ ì¸¡ì •ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
        E2E í…ŒìŠ¤íŠ¸ë¡œ ê²€ì¦í•˜ëŠ” ê²ƒì´ ì ì ˆí•©ë‹ˆë‹¤.

    Returns:
        (í…ŒìŠ¤íŠ¸ ìˆ˜, src ì»¤ë²„ë¦¬ì§€, ì‹¤í–‰ ì‹œê°„)
    """
    # pytest-covë¡œ src/ë§Œ ì»¤ë²„ë¦¬ì§€ ì¸¡ì • (scripts/ ì œì™¸)
    cmd = [
        sys.executable,
        "-m",
        "pytest",
        "--cov=src",
        "--cov-report=term",
        "-q",
        str(TESTS_DIR),
    ]

    _, stdout, _ = run_command(cmd)

    # í…ŒìŠ¤íŠ¸ ìˆ˜ íŒŒì‹± (ì˜ˆ: "42 passed in 1.23s")
    test_count = 0
    duration = 0.0
    match = re.search(r"(\d+) passed", stdout)
    if match:
        test_count = int(match.group(1))

    match = re.search(r"in ([\d.]+)s", stdout)
    if match:
        duration = float(match.group(1))

    # src/ ì»¤ë²„ë¦¬ì§€ (TOTAL ë¼ì¸ì—ì„œ íŒŒì‹±)
    coverage_src = 0.0
    match = re.search(r"TOTAL\s+\d+\s+\d+\s+(\d+)%", stdout)
    if match:
        coverage_src = float(match.group(1))

    return test_count, coverage_src, duration


def get_mypy_errors() -> int:
    """mypy ì‹¤í–‰í•˜ì—¬ ì—ëŸ¬ ìˆ˜ ì¹´ìš´íŠ¸.

    Returns:
        mypy ì—ëŸ¬ ìˆ˜.
    """
    cmd = [sys.executable, "-m", "mypy", str(SRC_DIR), str(SCRIPTS_DIR)]
    returncode, stdout, _ = run_command(cmd)

    if returncode == 0:
        return 0

    # ì—ëŸ¬ ë¼ì¸ ì¹´ìš´íŠ¸ (ì˜ˆ: "Found 3 errors in 2 files")
    match = re.search(r"Found (\d+) error", stdout)
    if match:
        return int(match.group(1))

    # ê°œë³„ ì—ëŸ¬ ë¼ì¸ ì¹´ìš´íŠ¸
    error_lines = [line for line in stdout.split("\n") if ": error:" in line]
    return len(error_lines)


def get_ruff_issues() -> int:
    """ruff ì‹¤í–‰í•˜ì—¬ ë¦°íŠ¸ ì´ìŠˆ ìˆ˜ ì¹´ìš´íŠ¸.

    Returns:
        ruff ì´ìŠˆ ìˆ˜.
    """
    cmd = [
        sys.executable,
        "-m",
        "ruff",
        "check",
        str(SRC_DIR),
        str(SCRIPTS_DIR),
        "--output-format=json",
    ]
    returncode, stdout, _ = run_command(cmd)

    if returncode == 0:
        return 0

    # stdoutì´ Noneì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ì²˜ë¦¬
    if not stdout:
        return 0

    try:
        issues = json.loads(stdout)
        return len(issues)
    except (json.JSONDecodeError, TypeError):
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¼ì¸ ìˆ˜ë¡œ ì¹´ìš´íŠ¸
        return len([line for line in stdout.split("\n") if line.strip()])


def get_complexity_report() -> list[FileComplexity]:
    """radonìœ¼ë¡œ Cyclomatic Complexity ì¸¡ì •.

    Note:
        run_benchmarks.pyëŠ” í‰ê°€ ë„êµ¬ì´ë¯€ë¡œ ë¶„ì„ ëŒ€ìƒì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.

    Returns:
        íŒŒì¼ë³„ ë³µì¡ë„ ë¦¬ìŠ¤íŠ¸.
    """
    results: list[FileComplexity] = []

    # src/ ë° scripts/ íŒŒì¼ë“¤ ë¶„ì„ (run_benchmarks.py ì œì™¸)
    target_files = list(SRC_DIR.rglob("*.py")) + [
        f for f in SCRIPTS_DIR.rglob("*.py") if f.name != "run_benchmarks.py"
    ]

    for file_path in target_files:
        if "__pycache__" in str(file_path):
            continue

        # radon cc ì‹¤í–‰
        cmd = [sys.executable, "-m", "radon", "cc", str(file_path), "-s", "-j"]
        _, stdout, _ = run_command(cmd)

        try:
            data = json.loads(stdout)
            if not data:
                continue

            file_data = data.get(str(file_path), [])
            if not file_data:
                continue

            # ë³µì¡ë„ ê³„ì‚°
            complexities = [item.get("complexity", 0) for item in file_data]
            avg_cc = sum(complexities) / len(complexities) if complexities else 0
            max_cc = max(complexities) if complexities else 0

            # ìµœê³  ë³µì¡ë„ í•¨ìˆ˜ ì°¾ê¸°
            max_item = max(file_data, key=lambda x: x.get("complexity", 0))
            max_func = max_item.get("name", "unknown")

            # ë¼ì¸ ìˆ˜ ê³„ì‚°
            line_count = len(file_path.read_text(encoding="utf-8").splitlines())

            results.append(
                FileComplexity(
                    file_path=str(file_path.relative_to(PROJECT_ROOT)),
                    line_count=line_count,
                    average_cc=round(avg_cc, 2),
                    max_cc=max_cc,
                    max_cc_function=f"{max_func} ({max_cc})",
                )
            )
        except (json.JSONDecodeError, KeyError):
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            line_count = len(file_path.read_text(encoding="utf-8").splitlines())
            results.append(
                FileComplexity(
                    file_path=str(file_path.relative_to(PROJECT_ROOT)),
                    line_count=line_count,
                    average_cc=0.0,
                    max_cc=0.0,
                    max_cc_function="N/A",
                )
            )

    # ë¼ì¸ ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    results.sort(key=lambda x: x.line_count, reverse=True)
    return results


def get_module_stats() -> tuple[int, float, int]:
    """src/pipeline/ ëª¨ë“ˆ í†µê³„.

    Returns:
        (íŒŒì¼ ìˆ˜, í‰ê·  ë¼ì¸ ìˆ˜, ì´ ë¼ì¸ ìˆ˜)
    """
    pipeline_dir = SRC_DIR / "pipeline"
    if not pipeline_dir.exists():
        return 0, 0.0, 0

    py_files = [f for f in pipeline_dir.glob("*.py") if f.name != "__init__.py"]
    if not py_files:
        return 0, 0.0, 0

    total_lines = sum(
        len(f.read_text(encoding="utf-8").splitlines()) for f in py_files
    )
    return len(py_files), round(total_lines / len(py_files), 1), total_lines


def get_maintainability_index() -> float:
    """radonìœ¼ë¡œ Maintainability Index ì¸¡ì •.

    MI ì ìˆ˜ í•´ì„:
        - 100-20: ìœ ì§€ë³´ìˆ˜ ìš©ì´
        - 19-10: ë³´í†µ
        - 9-0: ìœ ì§€ë³´ìˆ˜ ì–´ë ¤ì›€

    Returns:
        í‰ê·  Maintainability Index (0-100).
    """
    cmd = [sys.executable, "-m", "radon", "mi", str(SRC_DIR), "-s", "-j"]
    _, stdout, _ = run_command(cmd)

    if not stdout:
        return 0.0

    try:
        data = json.loads(stdout)
        if not data:
            return 0.0

        # ëª¨ë“  íŒŒì¼ì˜ MI í‰ê·  ê³„ì‚°
        mi_values = []
        for _, info in data.items():
            if isinstance(info, dict) and "mi" in info:
                mi_values.append(info["mi"])

        if not mi_values:
            return 0.0

        return round(sum(mi_values) / len(mi_values), 1)
    except (json.JSONDecodeError, KeyError):
        return 0.0


def get_function_stats(complexity_data: list[FileComplexity]) -> tuple[int, int, int]:
    """í•¨ìˆ˜ ê´€ë ¨ í†µê³„ ê³„ì‚°.

    Args:
        complexity_data: íŒŒì¼ë³„ ë³µì¡ë„ ì •ë³´ ë¦¬ìŠ¤íŠ¸.

    Returns:
        (ìµœëŒ€ í•¨ìˆ˜ ë¼ì¸ ìˆ˜, 50ì¤„ ì´ˆê³¼ í•¨ìˆ˜ ìˆ˜, ìµœëŒ€ CC)
    """
    max_lines = 0
    over_50_count = 0
    max_cc = 0

    # src/ íŒŒì¼ë“¤ë§Œ ë¶„ì„ (scripts/ ì œì™¸)
    for file_path in SRC_DIR.rglob("*.py"):
        if "__pycache__" in str(file_path):
            continue

        # radon rawë¡œ í•¨ìˆ˜ë³„ ë¼ì¸ ìˆ˜ ì¸¡ì •
        cmd = [sys.executable, "-m", "radon", "raw", str(file_path), "-j"]
        _, stdout, _ = run_command(cmd)

        try:
            data = json.loads(stdout)
            if not data:
                continue

            file_data = data.get(str(file_path), {})
            # LOC (Lines of Code) ì‚¬ìš©
            loc = file_data.get("loc", 0)
            if loc > max_lines:
                max_lines = loc
        except (json.JSONDecodeError, KeyError):
            pass

    # ë³µì¡ë„ ë°ì´í„°ì—ì„œ ìµœëŒ€ CC ë° 50ì¤„ ì´ˆê³¼ í•¨ìˆ˜ ê³„ì‚°
    for fc in complexity_data:
        # src/ íŒŒì¼ë§Œ (scripts/ ì œì™¸)
        if fc.file_path.startswith("src"):
            if fc.line_count > 50:
                over_50_count += 1
            if fc.max_cc > max_cc:
                max_cc = int(fc.max_cc)

    return max_lines, over_50_count, max_cc


# =============================================================================
# ë©”íŠ¸ë¦­ ìˆ˜ì§‘
# =============================================================================


def collect_metrics(tag: str) -> BenchmarkMetrics:
    """ëª¨ë“  ë©”íŠ¸ë¦­ ìˆ˜ì§‘.

    Args:
        tag: ë²¤ì¹˜ë§ˆí¬ íƒœê·¸ (ì˜ˆ: "baseline", "phase1").

    Returns:
        ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­.
    """
    print(f"ğŸ“Š ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘... (tag: {tag})")

    print("  â”œâ”€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    test_count, cov_src, duration = get_test_metrics()

    print("  â”œâ”€ mypy ê²€ì‚¬ ì¤‘...")
    mypy_errors = get_mypy_errors()

    print("  â”œâ”€ ruff ê²€ì‚¬ ì¤‘...")
    ruff_issues = get_ruff_issues()

    print("  â”œâ”€ ë³µì¡ë„ ë¶„ì„ ì¤‘...")
    complexity = get_complexity_report()

    print("  â”œâ”€ ìœ ì§€ë³´ìˆ˜ì„± ë¶„ì„ ì¤‘...")
    mi = get_maintainability_index()

    print("  â”œâ”€ í•¨ìˆ˜ í†µê³„ ë¶„ì„ ì¤‘...")
    max_func_lines, over_50_count, max_cc = get_function_stats(complexity)

    print("  â””â”€ ëª¨ë“ˆ í†µê³„ ìˆ˜ì§‘ ì¤‘...")
    file_count, avg_lines, total_lines = get_module_stats()

    return BenchmarkMetrics(
        timestamp=datetime.now().isoformat(),
        tag=tag,
        test_count=test_count,
        coverage_src=cov_src,
        mypy_errors=mypy_errors,
        ruff_issues=ruff_issues,
        complexity=complexity,
        src_file_count=file_count,
        avg_file_lines=avg_lines,
        total_src_lines=total_lines,
        max_function_lines=max_func_lines,
        functions_over_50_lines=over_50_count,
        max_function_cc=max_cc,
        maintainability_index=mi,
        test_duration_seconds=duration,
    )


# =============================================================================
# ì €ì¥ ë° ë Œë”ë§
# =============================================================================


def save_json(metrics: BenchmarkMetrics) -> Path:
    """ë©”íŠ¸ë¦­ì„ JSONìœ¼ë¡œ ì €ì¥.

    Args:
        metrics: ë²¤ì¹˜ë§ˆí¬ ë©”íŠ¸ë¦­.

    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ.
    """
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    date_str = datetime.now().strftime("%Y-%m-%d")
    file_path = DATA_DIR / f"{date_str}_{metrics.tag}.json"

    # dataclassë¥¼ dictë¡œ ë³€í™˜
    data = asdict(metrics)

    file_path.write_text(
        json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8"
    )
    return file_path


def render_markdown(metrics: BenchmarkMetrics) -> str:
    """ë©”íŠ¸ë¦­ì„ Markdownìœ¼ë¡œ ë Œë”ë§.

    Args:
        metrics: ë²¤ì¹˜ë§ˆí¬ ë©”íŠ¸ë¦­.

    Returns:
        Markdown ë¬¸ìì—´.
    """
    # ë³µì¡ë„ í…Œì´ë¸” ìƒì„±
    complexity_rows = ""
    for fc in metrics.complexity[:10]:  # ìƒìœ„ 10ê°œë§Œ
        complexity_rows += (
            f"| {fc.file_path} | {fc.line_count} | "
            f"{fc.average_cc} | {fc.max_cc_function} |\n"
        )

    # MI ë“±ê¸‰ ê³„ì‚°
    mi = metrics.maintainability_index
    if mi >= 20:
        mi_grade = "A (ìœ ì§€ë³´ìˆ˜ ìš©ì´)"
    elif mi >= 10:
        mi_grade = "B (ë³´í†µ)"
    else:
        mi_grade = "C (ìœ ì§€ë³´ìˆ˜ ì–´ë ¤ì›€)"

    return f"""# Benchmark Report - {metrics.tag}

> ìƒì„± ì‹œê°„: {metrics.timestamp}

## 1. í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­

- ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: **{metrics.test_count}ê°œ**
- src/ ì»¤ë²„ë¦¬ì§€: **{metrics.coverage_src:.1f}%**
- í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œê°„: **{metrics.test_duration_seconds:.2f}ì´ˆ**

## 2. ì½”ë“œ í’ˆì§ˆ ë©”íŠ¸ë¦­

- mypy ì—ëŸ¬: **{metrics.mypy_errors}ê°œ**
- ruff ì´ìŠˆ: **{metrics.ruff_issues}ê°œ**
- Maintainability Index: **{mi:.1f}** ({mi_grade})

## 3. ì½”ë“œ ë³µì¡ë„

| íŒŒì¼ | ë¼ì¸ ìˆ˜ | í‰ê·  CC | ìµœê³  CC í•¨ìˆ˜ |
| ---- | ------- | ------- | ------------ |
{complexity_rows}

## 4. ë¦¬íŒ©í† ë§ ì§€í‘œ

| ì§€í‘œ | ê°’ | ëª©í‘œ |
| ---- | -- | ---- |
| ìµœëŒ€ í•¨ìˆ˜ CC | {metrics.max_function_cc} | < 10 |
| 50ì¤„ ì´ˆê³¼ íŒŒì¼ ìˆ˜ | {metrics.functions_over_50_lines} | 0 |
| src/ ì´ ë¼ì¸ ìˆ˜ | {metrics.total_src_lines} | - |

## 5. ëª¨ë“ˆ êµ¬ì¡°

- src/pipeline/ íŒŒì¼ ìˆ˜: **{metrics.src_file_count}ê°œ**
- í‰ê·  íŒŒì¼ í¬ê¸°: **{metrics.avg_file_lines}ì¤„**

## 6. ìš”ì•½

| ì§€í‘œ | ê°’ | ìƒíƒœ |
| ---- | -- | ---- |
| í…ŒìŠ¤íŠ¸ ìˆ˜ | {metrics.test_count} | - |
| ì»¤ë²„ë¦¬ì§€ (src/) | {metrics.coverage_src}% | {'âœ…' if metrics.coverage_src >= 80 else 'âš ï¸'} |
| mypy ì—ëŸ¬ | {metrics.mypy_errors} | {'âœ…' if metrics.mypy_errors == 0 else 'âš ï¸'} |
| ruff ì´ìŠˆ | {metrics.ruff_issues} | {'âœ…' if metrics.ruff_issues == 0 else 'âš ï¸'} |
| Maintainability Index | {mi:.1f} | {'âœ…' if mi >= 20 else 'âš ï¸'} |
| ê°€ì¥ í° íŒŒì¼ | {metrics.complexity[0].file_path if metrics.complexity else 'N/A'} ({metrics.complexity[0].line_count if metrics.complexity else 0}ì¤„) | - |
"""


def save_markdown(metrics: BenchmarkMetrics) -> Path:
    """ë©”íŠ¸ë¦­ì„ Markdownìœ¼ë¡œ ì €ì¥.

    Args:
        metrics: ë²¤ì¹˜ë§ˆí¬ ë©”íŠ¸ë¦­.

    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ.
    """
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)

    date_str = datetime.now().strftime("%Y-%m-%d")
    file_path = REPORTS_DIR / f"{date_str}_{metrics.tag}.md"

    content = render_markdown(metrics)
    file_path.write_text(content, encoding="utf-8")
    return file_path


# =============================================================================
# ë¹„êµ ê¸°ëŠ¥
# =============================================================================


def load_metrics(tag: str) -> BenchmarkMetrics | None:
    """ì €ì¥ëœ ë©”íŠ¸ë¦­ ë¡œë“œ (ê°€ì¥ ìµœê·¼ íŒŒì¼).

    Args:
        tag: ë²¤ì¹˜ë§ˆí¬ íƒœê·¸.

    Returns:
        ë©”íŠ¸ë¦­ ë˜ëŠ” None.
    """
    pattern = f"*_{tag}.json"
    files = sorted(DATA_DIR.glob(pattern), reverse=True)

    if not files:
        print(f"âš ï¸  '{tag}' íƒœê·¸ì˜ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    data = json.loads(files[0].read_text(encoding="utf-8"))

    # complexity í•„ë“œë¥¼ FileComplexity ê°ì²´ë¡œ ë³€í™˜
    complexity_list = [FileComplexity(**c) for c in data.get("complexity", [])]

    return BenchmarkMetrics(
        timestamp=data["timestamp"],
        tag=data["tag"],
        test_count=data["test_count"],
        coverage_src=data.get("coverage_src", data.get("coverage_total", 0.0)),
        mypy_errors=data["mypy_errors"],
        ruff_issues=data["ruff_issues"],
        complexity=complexity_list,
        src_file_count=data.get("src_file_count", 0),
        avg_file_lines=data.get("avg_file_lines", 0.0),
        total_src_lines=data.get("total_src_lines", 0),
        max_function_lines=data.get("max_function_lines", 0),
        functions_over_50_lines=data.get("functions_over_50_lines", 0),
        max_function_cc=data.get("max_function_cc", 0),
        maintainability_index=data.get("maintainability_index", 0.0),
        test_duration_seconds=data.get("test_duration_seconds", 0.0),
    )


def compare_metrics(before_tag: str, after_tag: str) -> None:
    """ë‘ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±.

    Args:
        before_tag: ì´ì „ ë²¤ì¹˜ë§ˆí¬ íƒœê·¸.
        after_tag: ì´í›„ ë²¤ì¹˜ë§ˆí¬ íƒœê·¸.
    """
    before = load_metrics(before_tag)
    after = load_metrics(after_tag)

    if not before or not after:
        return

    def delta(b: float, a: float) -> str:
        diff = a - b
        if diff > 0:
            return f"+{diff:.1f}"
        elif diff < 0:
            return f"{diff:.1f}"
        return "0"

    def delta_int(b: int, a: int) -> str:
        diff = a - b
        if diff > 0:
            return f"+{diff}"
        elif diff < 0:
            return f"{diff}"
        return "0"

    report = f"""# Comparison Report: {before_tag} â†’ {after_tag}

> ìƒì„± ì‹œê°„: {datetime.now().isoformat()}

## ë©”íŠ¸ë¦­ ë³€í™”

| ì§€í‘œ | {before_tag} | {after_tag} | ë³€í™” |
| ---- | ------------ | ----------- | ---- |
| í…ŒìŠ¤íŠ¸ ìˆ˜ | {before.test_count} | {after.test_count} | {delta_int(before.test_count, after.test_count)} |
| ì»¤ë²„ë¦¬ì§€ (src/) | {before.coverage_src}% | {after.coverage_src}% | {delta(before.coverage_src, after.coverage_src)}% |
| mypy ì—ëŸ¬ | {before.mypy_errors} | {after.mypy_errors} | {delta_int(before.mypy_errors, after.mypy_errors)} |
| ruff ì´ìŠˆ | {before.ruff_issues} | {after.ruff_issues} | {delta_int(before.ruff_issues, after.ruff_issues)} |
| Maintainability Index | {before.maintainability_index} | {after.maintainability_index} | {delta(before.maintainability_index, after.maintainability_index)} |
| ìµœëŒ€ í•¨ìˆ˜ CC | {before.max_function_cc} | {after.max_function_cc} | {delta_int(before.max_function_cc, after.max_function_cc)} |
| 50ì¤„ ì´ˆê³¼ íŒŒì¼ | {before.functions_over_50_lines} | {after.functions_over_50_lines} | {delta_int(before.functions_over_50_lines, after.functions_over_50_lines)} |
| src íŒŒì¼ ìˆ˜ | {before.src_file_count} | {after.src_file_count} | {delta_int(before.src_file_count, after.src_file_count)} |
| í‰ê·  íŒŒì¼ í¬ê¸° | {before.avg_file_lines}ì¤„ | {after.avg_file_lines}ì¤„ | {delta(before.avg_file_lines, after.avg_file_lines)}ì¤„ |

## ê°œì„  ì—¬ë¶€

"""

    # ê°œì„  íŒë‹¨
    improvements = []
    regressions = []

    if after.coverage_src > before.coverage_src:
        improvements.append(
            f"âœ… ì»¤ë²„ë¦¬ì§€ í–¥ìƒ: {before.coverage_src}% â†’ {after.coverage_src}%"
        )
    elif after.coverage_src < before.coverage_src:
        regressions.append(
            f"âš ï¸ ì»¤ë²„ë¦¬ì§€ ê°ì†Œ: {before.coverage_src}% â†’ {after.coverage_src}%"
        )

    if after.maintainability_index > before.maintainability_index:
        improvements.append(
            f"âœ… ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ: {before.maintainability_index} â†’ {after.maintainability_index}"
        )

    if after.max_function_cc < before.max_function_cc:
        improvements.append(
            f"âœ… ìµœëŒ€ ë³µì¡ë„ ê°ì†Œ: {before.max_function_cc} â†’ {after.max_function_cc}"
        )

    if after.functions_over_50_lines < before.functions_over_50_lines:
        improvements.append(
            f"âœ… ê¸´ íŒŒì¼ ê°ì†Œ: {before.functions_over_50_lines} â†’ {after.functions_over_50_lines}"
        )

    if after.mypy_errors < before.mypy_errors:
        improvements.append(
            f"âœ… mypy ì—ëŸ¬ ê°ì†Œ: {before.mypy_errors} â†’ {after.mypy_errors}"
        )
    elif after.mypy_errors > before.mypy_errors:
        regressions.append(
            f"âš ï¸ mypy ì—ëŸ¬ ì¦ê°€: {before.mypy_errors} â†’ {after.mypy_errors}"
        )

    if after.ruff_issues < before.ruff_issues:
        improvements.append(
            f"âœ… ruff ì´ìŠˆ ê°ì†Œ: {before.ruff_issues} â†’ {after.ruff_issues}"
        )
    elif after.ruff_issues > before.ruff_issues:
        regressions.append(
            f"âš ï¸ ruff ì´ìŠˆ ì¦ê°€: {before.ruff_issues} â†’ {after.ruff_issues}"
        )

    if improvements:
        report += (
            "### ê°œì„ ëœ í•­ëª©\n\n" + "\n".join(f"- {i}" for i in improvements) + "\n\n"
        )

    if regressions:
        report += (
            "### ì£¼ì˜ í•„ìš” í•­ëª©\n\n" + "\n".join(f"- {r}" for r in regressions) + "\n\n"
        )

    if not improvements and not regressions:
        report += "ë³€í™” ì—†ìŒ.\n"

    # ì €ì¥
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    date_str = datetime.now().strftime("%Y-%m-%d")
    file_path = REPORTS_DIR / f"{date_str}_compare_{before_tag}_vs_{after_tag}.md"
    file_path.write_text(report, encoding="utf-8")

    print(f"âœ… ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±: {file_path}")


# =============================================================================
# CLI
# =============================================================================


def create_parser() -> argparse.ArgumentParser:
    """CLI íŒŒì„œ ìƒì„±."""
    parser = argparse.ArgumentParser(
        description="ë¦¬íŒ©í† ë§ ë²¤ì¹˜ë§ˆí¬ ì¸¡ì • ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  uv run scripts/run_benchmarks.py baseline
  uv run scripts/run_benchmarks.py snapshot phase1
  uv run scripts/run_benchmarks.py compare baseline phase1
        """,
    )

    subparsers = parser.add_subparsers(dest="command", help="ëª…ë ¹ì–´")

    # baseline ëª…ë ¹ì–´
    subparsers.add_parser("baseline", help="ë¦¬íŒ©í† ë§ ì „ ê¸°ì¤€ì„  ìº¡ì²˜")

    # snapshot ëª…ë ¹ì–´
    snapshot_parser = subparsers.add_parser("snapshot", help="í˜„ì¬ ìƒíƒœ ìŠ¤ëƒ…ìƒ·")
    snapshot_parser.add_argument("tag", help="ìŠ¤ëƒ…ìƒ· íƒœê·¸ (ì˜ˆ: phase1, final)")

    # compare ëª…ë ¹ì–´
    compare_parser = subparsers.add_parser("compare", help="ë‘ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ")
    compare_parser.add_argument("before", help="ì´ì „ ë²¤ì¹˜ë§ˆí¬ íƒœê·¸")
    compare_parser.add_argument("after", help="ì´í›„ ë²¤ì¹˜ë§ˆí¬ íƒœê·¸")

    return parser


def main() -> int:
    """ë©”ì¸ ì§„ì…ì .

    Returns:
        ì¢…ë£Œ ì½”ë“œ.
    """
    parser = create_parser()
    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return 1

    if args.command == "baseline":
        metrics = collect_metrics("baseline")
        json_path = save_json(metrics)
        md_path = save_markdown(metrics)
        print("\nâœ… Baseline ì €ì¥ ì™„ë£Œ:")
        print(f"   - JSON: {json_path}")
        print(f"   - Markdown: {md_path}")

    elif args.command == "snapshot":
        metrics = collect_metrics(args.tag)
        json_path = save_json(metrics)
        md_path = save_markdown(metrics)
        print(f"\nâœ… ìŠ¤ëƒ…ìƒ· ì €ì¥ ì™„ë£Œ (tag: {args.tag}):")
        print(f"   - JSON: {json_path}")
        print(f"   - Markdown: {md_path}")

    elif args.command == "compare":
        compare_metrics(args.before, args.after)

    return 0


if __name__ == "__main__":
    sys.exit(main())
