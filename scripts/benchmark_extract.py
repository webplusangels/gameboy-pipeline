"""
Extract ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸.

ìˆœì°¨ ì¶”ì¶œì˜ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
S3 ì ì¬ ì—†ì´ ìˆœìˆ˜ API ì¶”ì¶œ ì‹œê°„ë§Œ ì¸¡ì •í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ì „ì²´ ì—”í‹°í‹° ì¸¡ì •
    uv run scripts/benchmark_extract.py

    # íŠ¹ì • ì—”í‹°í‹°ë§Œ ì¸¡ì •
    uv run scripts/benchmark_extract.py --entity games

    # ë°˜ë³µ íšŸìˆ˜ ì§€ì •
    uv run scripts/benchmark_extract.py --entity platforms --runs 3
"""

import argparse
import asyncio
import json
import sys
from dataclasses import asdict, dataclass
from datetime import datetime
from pathlib import Path
from time import perf_counter

import httpx
from loguru import logger

from src.config import settings
from src.pipeline.auth import StaticAuthProvider
from src.pipeline.registry import ALL_ENTITIES


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤."""

    entity_name: str
    record_count: int
    elapsed_seconds: float
    records_per_second: float
    api_calls: int
    timestamp: str


async def benchmark_entity(
    entity_name: str,
    extractor_cls: type,
    http_client: httpx.AsyncClient,
    auth_provider: StaticAuthProvider,
    client_id: str,
) -> BenchmarkResult:
    """
    ë‹¨ì¼ ì—”í‹°í‹°ì˜ ì¶”ì¶œ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

    Args:
        entity_name: ì—”í‹°í‹° ì´ë¦„
        extractor_cls: Extractor í´ë˜ìŠ¤
        http_client: HTTP í´ë¼ì´ì–¸íŠ¸
        auth_provider: ì¸ì¦ ì œê³µì
        client_id: IGDB í´ë¼ì´ì–¸íŠ¸ ID

    Returns:
        BenchmarkResult: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
    """
    extractor = extractor_cls(
        client=http_client,
        auth_provider=auth_provider,
        client_id=client_id,
    )

    logger.info(f"[{entity_name}] ì¶”ì¶œ ì‹œì‘...")
    start_time = perf_counter()

    record_count = 0
    api_calls = 0

    async for _ in extractor.extract():
        record_count += 1
        # í˜ì´ì§€ ê²½ê³„ì—ì„œ API í˜¸ì¶œ íšŸìˆ˜ ì¶”ì •
        if record_count % extractor.limit == 1:
            api_calls += 1

    elapsed = perf_counter() - start_time
    records_per_second = record_count / elapsed if elapsed > 0 else 0

    result = BenchmarkResult(
        entity_name=entity_name,
        record_count=record_count,
        elapsed_seconds=round(elapsed, 2),
        records_per_second=round(records_per_second, 2),
        api_calls=api_calls,
        timestamp=datetime.now().isoformat(),
    )

    logger.success(
        f"[{entity_name}] ì™„ë£Œ: {record_count:,}ê°œ / {elapsed:.2f}ì´ˆ "
        f"({records_per_second:.2f} rec/sec, {api_calls} API calls)"
    )

    return result


async def run_benchmark(
    entities: list[str] | None = None,
    runs: int = 1,
) -> list[BenchmarkResult]:
    """
    ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        entities: ì¸¡ì •í•  ì—”í‹°í‹° ëª©ë¡ (Noneì´ë©´ ì „ì²´)
        runs: ë°˜ë³µ íšŸìˆ˜

    Returns:
        list[BenchmarkResult]: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ëª©ë¡
    """
    client_id = settings.igdb_client_id
    static_token = settings.igdb_static_token

    if not client_id or not static_token:
        logger.error("IGDB_CLIENT_ID ë˜ëŠ” IGDB_STATIC_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    auth_provider = StaticAuthProvider(token=static_token)

    target_entities = entities or list(ALL_ENTITIES.keys())
    all_results: list[BenchmarkResult] = []

    async with httpx.AsyncClient(timeout=30.0) as http_client:
        for run_num in range(1, runs + 1):
            if runs > 1:
                logger.info(f"=== Run {run_num}/{runs} ===")

            for entity_name in target_entities:
                if entity_name not in ALL_ENTITIES:
                    logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì—”í‹°í‹°: {entity_name}")
                    continue

                extractor_cls = ALL_ENTITIES[entity_name]
                result = await benchmark_entity(
                    entity_name=entity_name,
                    extractor_cls=extractor_cls,
                    http_client=http_client,
                    auth_provider=auth_provider,
                    client_id=client_id,
                )
                all_results.append(result)

    return all_results


def save_results(results: list[BenchmarkResult], output_path: Path) -> None:
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    data = {
        "benchmark_type": "extract_sequential",
        "timestamp": datetime.now().isoformat(),
        "environment": {
            "python_version": sys.version,
            "platform": sys.platform,
        },
        "results": [asdict(r) for r in results],
        "summary": {
            "total_records": sum(r.record_count for r in results),
            "total_elapsed_seconds": round(sum(r.elapsed_seconds for r in results), 2),
            "total_api_calls": sum(r.api_calls for r in results),
        },
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

    logger.info(f"ê²°ê³¼ ì €ì¥: {output_path}")


def print_summary(results: list[BenchmarkResult]) -> None:
    """ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "=" * 60)
    print("ğŸ“Š Extract ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ (ìˆœì°¨ ì²˜ë¦¬)")
    print("=" * 60)
    print(f"{'ì—”í‹°í‹°':<25} {'ë ˆì½”ë“œ':>10} {'ì‹œê°„(ì´ˆ)':>10} {'rec/sec':>10}")
    print("-" * 60)

    for r in results:
        print(
            f"{r.entity_name:<25} {r.record_count:>10,} {r.elapsed_seconds:>10.2f} {r.records_per_second:>10.2f}"
        )

    print("-" * 60)
    total_records = sum(r.record_count for r in results)
    total_time = sum(r.elapsed_seconds for r in results)
    avg_rps = total_records / total_time if total_time > 0 else 0

    print(f"{'ì´ê³„':<25} {total_records:>10,} {total_time:>10.2f} {avg_rps:>10.2f}")
    print("=" * 60)


def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜."""
    parser = argparse.ArgumentParser(description="Extract ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    parser.add_argument(
        "--entity",
        type=str,
        help="ì¸¡ì •í•  ì—”í‹°í‹° ì´ë¦„ (ì˜ˆ: games, platforms). ë¯¸ì§€ì •ì‹œ ì „ì²´ ì¸¡ì •",
    )
    parser.add_argument(
        "--runs",
        type=int,
        default=1,
        help="ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 1)",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì˜ˆ: .benchmarks/results/extract_baseline.json)",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="ìƒì„¸ ë¡œê·¸ ìˆ¨ê¸°ê¸°",
    )
    args = parser.parse_args()

    # ë¡œê¹… ì„¤ì •
    logger.remove()
    if not args.quiet:
        logger.add(sys.stderr, level="INFO")
    else:
        logger.add(sys.stderr, level="WARNING")

    # ì—”í‹°í‹° ëª©ë¡
    entities = [args.entity] if args.entity else None

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    logger.info("=== Extract ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ===")
    results = asyncio.run(run_benchmark(entities=entities, runs=args.runs))

    # ê²°ê³¼ ì¶œë ¥
    print_summary(results)

    # ê²°ê³¼ ì €ì¥
    if args.output:
        save_results(results, Path(args.output))
    else:
        # ê¸°ë³¸ ê²½ë¡œì— ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        default_path = Path(
            f"docs/refactoring/benchmarks/data/extract_sequential_{timestamp}.json"
        )
        save_results(results, default_path)


if __name__ == "__main__":
    main()
