# 03. 학습 및 문제 해결 로그

이 문서는 프로젝트를 진행하며 마주친 주요 기술적 문제와 이를 해결한 과정을 기록합니다. 실제 제가 고민한 문제들에 대해 순차적으로 기술하겠습니다.

<!-- [Architecture] - 설계 관련
[Testing] - 테스트 작성/실행
[CI/CD] - GitHub Actions
[Tooling] - uv, ruff, mypy
[Python] - 언어 자체 이슈 -->

## [Architecture] TDD

2025-10-31

### 문제

> [!WARNING]
> TDD의 자세한 구현 방식을 모른다!

이 프로젝트를 시작하기 위해 목표로 삼은 것은 작지만 현대적인 요소가 모두 들어갈 수 있는 빠르고 확장성 있는 파이프라인 만들기였습니다.

이 정의에 가장 알맞는 개발 방법론은 테스트를 먼저 작성한 뒤 테스트 통과에 필요한 기능들을 순차적으로 개발하는 단순하며 문제 해결 중심의 **TDD**라고 생각했습니다.

문제가 있었다면 제가 TDD에 대해 이론적으로, 혹은 구현의 극히 일부만 알고 있었다는 사실입니다.

### 해결

`Gemini`와 `Github Copilot`의 도움을 받아 [`CONTRIBUTING.md`](../CONTRIBUTING.md) 문서를 통해 가이드를 만들었습니다 (~~기승전AI~~). 기초적인 순서를 제가 개발하는 코드에 적용한다고 가정하면 다음과 같습니다.

1. **🔴 RED: 실패하는 테스트 작성**

   - `tests/` 디렉터리에 새 기능에 대한 테스트 코드(`test_*.py`)를 작성합니다.
   - `mocker`를 사용해 외부 의존성(API, S3)을 철저히 모킹(Mocking)합니다.
   - `pytest`를 실행하여 **테스트가 예상대로 실패하는 것을 확인**합니다.

2. **🟢 GREEN: 테스트를 통과하는 최소한의 코드 작성**

   - `src/` 디렉터리에 `RED` 단계의 테스트를 **겨우 통과할 만큼의 최소한의 코드**를 작성합니다.
   - `pytest`를 실행하여 **모든 테스트가 통과하는 것을 확인**합니다.

3. **🟡 REFACTOR: 코드 개선**
   - 테스트가 통과하는 "안전망" 위에서 코드의 구조를 개선하고, 중복을 제거하며, 가독성을 높입니다.
   - 리팩토링 후에도 `pytest`를 실행하여 **모든 테스트가 계속 통과하는지 확인**합니다.

**RED** -> **GREEN** -> **REFACTOR** 각 단계에서 커밋을 반복하고, 기능 개발이 완료되면 이 브랜치를 push하게 됩니다. main 브랜치에 merge하는 과정에서 추가로 Github Actions의 CI 파이프라인이 다시 테스트를 실행하게 되는데요. 이는 뒤의 CI 파이프라인 작성 문제에서 자세히 설명합니다.

TDD에서 Git Branch 전략은 보통 'Squash and Merge'를 사용하는데, 쉽게 말해 브랜치를 단순히 합치는게 아닌 기능 개발 과정에서 진행한 커밋의 이력들을 main 브랜치에 하나의 새로운 커밋 형태로 합치는 형태입니다. 자잘한 TDD 히스토리를 기능 단위인 하나의 커밋으로 병합되는 것이죠.

또 하나 중요한건 TDD는 **외부 환경과 분리된 순수한 로직 테스트를 지향**한다는 것입니다. 현재의 데이터 파이프라인은 서드파티 API(IGDB)를 사용하고 있고, 이 API는 완전한 오픈소스에다 Amazon에서 운영하고 있는 만큼 안정성은 높지만 테스트 과정에서는 모킹된 샘플 데이터를 사용하는 것이 좋다고 생각합니다. 이를 위해 실제 IGDB API 응답을 기록해 모킹 응답을 저장하고 이를 테스트에 쓰는 방안을 고려 중입니다.

### 결과

이 과정을 통해 실제로 어떤 과정을 통해 TDD가 진행되는지 배울 수 있었습니다. 이 개발 플로우는 프로젝트 종료까지 계속해서 반복하게 되는 만큼, 선 테스트 후 구현 과정이 익숙하지는 않지만 익숙해져야 할 필요가 있다고 느꼈습니다.

현재는 ETL의 `extractors` 개발 중, 에러 케이스를 포함한 5개의 기본적인 테스트를 통과하는 코드를 구현하고, 이를 리팩토링하는 과정에 있습니다.

## [CI/CD] Github Actions CI 파이프라인

2025-11-01

### 문제

> [!WARNING]
> CI 파이프라인 작성의 효율적이고 필수적인 방법을 모른다!

처음 작성한 CI 워크플로우는 다음과 같은 항목을 포함하고 있었습니다.

- Python 단일 버전
- pytest만 실행

하지만 이 워크플로우는 `pyproject.toml`에서 기술한 `>=3.11`을 준수하지 않았고, 포맷팅, 린팅 에러 등을 포함할 수 있었습니다.

### 해결

이번에도 베스트 프랙티스를 찾기 위해 AI와 개선안을 논의하며 다음과 같은 항목을 포함하는 구조로 변경하였습니다.

- Python 버전 매트릭스로 3개 버전 동시 테스트 (3.11, 3.12, 3.13)
- `uv` 캐싱으로 CI 속도 향상
- `--extra dev`로 개발 의존성 명시적 설치
- 환경 변수 주입
- Codecov 업로드로 커버리지 추적
- 포맷팅/린팅과 테스트 분리해 병렬 실행

이를 포함한 워크플로우는 다음과 같습니다.

```yaml
name: CI with uv

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]

jobs:
  lint:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Set up uv
        uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true

      - name: Install dependencies with uv
        run: |
          uv venv
          uv sync --extra dev

      - name: Run Ruff linter
        run: |
          uv run ruff check src tests

      - name: Check code formatting
        run: |
          uv run ruff format --check src tests

      - name: Run type checker with uv
        run: |
          uv run mypy src

  test:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.11", "3.12", "3.13"]

    steps:
      # 코드 체크아웃
      - uses: actions/checkout@v4

      # Python 설정
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      # 의존성 설치 - uv
      - name: Set up uv
        uses: astral-sh/setup-uv@v1
        with:
          enable-cache: true

      # 의존성 설치 - 프로젝트
      - name: Install dependencies with uv
        run: |
          uv venv
          uv sync --extra dev

      # 테스트 실행
      - name: Run tests with uv
        env:
          IGDB_CLIENT_ID: "test-client-id"
          IGDB_CLIENT_SECRET: "test-client-secret"
          IGDB_RATE_LIMIT: 4
          LOG_LEVEL: "INFO"
        run: |
          uv run pytest --cov=src --cov-report=xml

      # 코드 커버리지 업로드
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: matrix.python-version == '3.11'
        with:
          files: ./coverage.xml
          flags: unittests
          name: codecov-py${{ matrix.python-version }}
          fail_ci_if_error: false
```

### 결과

이번 작업에서 배운 점은 다음과 같습니다.

**1. 병렬 실행**

Github Actions에서 병렬 실행이 가능한거 알고 계셨나요?

**2. 캐싱**

캐싱으로 CI 시간을 단축할 수 있다는 사실도 처음 알았습니다. Github Actions가 가상 머신을 쓸 것이라는 추측을 어렴풋이 했지만 이런 기능도 가능하다니...

**3. 매트릭스 전략**

Python 버전 병렬 테스트로 호환성도 보장할 수 있습니다.

**4. CodeCov**

간단한 설정으로 현 코드의 테스트 커버리지 보고서를 업로드하고 시각적으로 확인할 수 있는 서비스입니다.

하다못해 Github Actions 워크플로우 파일도 최적화하는데 고려해야 할 점이 이렇게 많네요. 특히 가상 환경으로 `uv`를 처음 쓰면서 새로 알게 된 사실이 많았습니다.

## [Architecture] 추상 클래스 ABC

2025-11-01

### 문제

> [!WARNING]
> 의존성 역전 원칙(DIP)을 구현하는 방법을 모른다!

의존성 역전 원칙(DIP)은 다음과 같습니다.

> 고수준 모듈은 저수준 모듈에 의존해서는 안 된다. 둘 모두 추상화(인터페이스)에 의존해야 한다.

Python에서는 추상 클래스(인터페이스)를 어떻게 정의할 수 있을까요?

### 해결

`abc` 라이브러리의 `ABC`와 `abstactmethod`를 통해 쉽게 구현할 수 있습니다. 쉽게 말해 클래스가 `ABC`를 상속하면 해당 클래스를 추상 클래스라고 선언하고, 내부 메서드에 `@abstractmethod` 데코레이터를 붙이면 이 추상 클래스를 상속하는 자식 클래스들 내부에서 반드시 구현해야 하는 메서드임을 표현합니다. 예시는 다음과 같습니다.

```python
class Extractor(ABC):
    @abstractmethod
    async def extract(self) -> AsyncGenerator[dict[str, Any], None]:
        pass
```

추가로 `AsyncGenerator`는 비동기 제너레이터를 타입 힌트로 표현하는 방식입니다. `AsyncGenerator[dict[str, Any], None]`는 `yield`(제너레이터 반환 키워드)로 반환하는 값의 타입은 `dict[str, Any]`, 제너레이터 내부로 주입하는 값의 타입이 `None`이라는 뜻입니다.

추상 클래스와 비슷한 방식으로 작동하는 문법으로 `Protocols`이 존재하며 이는 클래스 상속이 아닌 덕타이핑 방식으로 작동합니다. 자세한 내용은 [문서](https://typing.python.org/en/latest/spec/protocol.html)를 참조해주세요.

### 결과

저는 ABC를 선택하였습니다. 물론 `Protocols`가 편리한 덕 타이핑을 제공하지만, 이 프로젝트에서는 `Extractor(ABC)`처럼 명시적인 상속을 사용하는 것이 TDD의 서브 클래스 검증이나 엄격한 타입 추론에서 안정적이고 명확하다는 판단 때문입니다.

이 과정을 통해 Python에서의 추상 클래스 구현에 대해 자세히 알아볼 수 있었습니다. 추상 클래스는 특정 규모 이상의 소프트웨어를 설계할 때 빠지지 않는 존재임을 체감했습니다. 추상클래스는 또한 의존성 역전을 구현하는 가장 명확한 방법이기도 합니다.

현재 ELT의 핵심 클래스들을 추상 클래스를 통해 안정적으로 구현하기 위한 과정에 있습니다.

## [Design] AuthProvider의 책임 범위

2025-11-02

### 문제

> [!WARNING]
> 환경 변수 등을 관리하는 인증 클래스는 어디까지 책임을 가지고 있는가?

ELT의 첫 모듈인 `Extractor`를 구현하던 중, Extractor는 인증을 필요로 한다는 사실을 깨달았습니다.

IGDB는 `Client ID`와 `Client Secret`을 통해 OAuth 토큰을 발급한 뒤, API 요청을 보내야 합니다. 두 값을 환경 변수로 설정한 상황에서 OAuth 토큰은 TTL이 설정되어 오기 때문에, 이를 관리할 새로운 클래스가 필요하며 이를 `AuthProvider`로 구현하기로 했습니다.

현재 상황에서 사용하는 방식은 복잡할 필요 없이 액세스 토큰을 직접 주입하고 반환하는 식으로 `AuthProvider` 인터페이스 - `StaticAuthProvider` 클래스 순서로 구현합니다. 이를 통해 현 코드 또한 테스트 코드와 의존성 없이 분리할 수 있고, 차후에 Redis와 같은 애플리케이션으로 실시간 토큰 발급과 재사용 여부를 확인할 수 있는 복잡한 Provider를 구현하게 되더라도 Extractor의 코드를 변경할 필요가 없어집니다.

하지만 client_id와 같은 값은 언제 넣어야 하는 걸까요? 실제 AuthProvider의 책임 범위는 어디까지여야 할지 궁금해졌습니다.

### 해결

`AuthProvider`는 토큰만 제공하는 것이 좋겠다고 결론내렸습니다.

일단 client_id와 같은 값은 IGDB API 설정이지, 인증 정보가 아닙니다. 그리고 `AuthProvider`에 더 많은 값을 두게 될 수록 해당 모듈의 의존성과 결합도가 높아지기 때문에 일반적으로도 여러 값을 관리하는 것은 바람직하지 않은 방향이기도 합니다.

그리고 본래 client_id는 환경 변수에 설정이 되어 있으므로 모듈이 환경 변수를 몰라도 되며 실제 호출 코드에서 지정해주는 것이 자연스럽지요.

### 결과

일단 모든 테스트를 GREEN으로 만든 후에 client_id를 `AuthProvider` 부분에서 삭제함으로서, Extractor는 직접 client_id를 받게 됩니다. 그렇게 테스트 코드에서도 완전한 의존성 분리와, 해당 테스트 코드에서 테스트 해야만 하는 부분을 좀 더 쉽게 인식할 수 있었습니다.

## [Architecture] Loader의 설계

2025-11-03

### 문제

> [!WARNING]
> Loader의 두 가지 방식: list vs generator

처음엔 `Loader`의 인터페이스를 다음과 같이 설계했습니다.

```python
class Loader(ABC):
    """
    Loader 인터페이스.

    이 인터페이스는 비동기적으로 데이터를 로드하는 메서드를 정의합니다.
    """

    @abstractmethod
    async def load(self, data: list[dict[str, Any]], key: str) -> None:
        """
        데이터 배치를 'key'라는 이름으로 Data Lake에 적재합니다.

        Args:
            data (list[dict[str, Any]]): Extractor가 생성한 데이터 배치.
            key (str): S3 등 데이터가 적재될 위치를 나타내는 키.
        """
        raise NotImplementedError
        return None

```

그러나 코드를 보던 중 이런 의문이 들었습니다.

> `data`가 배치(Batch)가 아니라면?

이렇게 구현하면 예상되는 문제점들은 다음과 같습니다.

```plain
❌ 메모리 압박: 10만 개 데이터를 list로 받으면 메모리 부족
❌ 유연성 부족: 스트리밍 처리 불가능
❌ Extractor와 불일치: Extractor는 AsyncGenerator인데 Loader는 list
```

그렇게 이 문제들을 해결하기 위해 제시된 대안은 **스트리밍** 방식으로 `Loader`역시 `AsyncGenerator`를 이용해 좀 더 유연한 방식으로 대체하는 것이었습니다. 과연 둘의 트레이드 오프를 고려했을 때 `Loader`의 알맞은 구조는 무엇일까요?

### 해결

저의 결론은 `list` 인터페이스가 더 실용적인 설계라는 것이었습니다.

일단 제 파이프라인에서 `Loader`는 S3와 같은 파일 기반의 데이터 레이크에 적재를 목적으로 하고 있습니다. 만약 스트리밍 방식으로 이를 구현하게 되면 Extract 단계를 포함해 O(1)의 메모리를 사용하면서 파이프라인 전체가 흐르는-스트리밍되며 좋은 설계 구조가 되겠지만, `Loader`의 복잡도는 매우 높아지게 됩니다.

예를 들어 S3는 파일 단위이며, 스트림을 파일로 만들기 위해서는 `Loader`내부에 업로드 로직이나 내부에서 배치 처리를 직접 구현해야 하는 복잡성이 추가됩니다. 이는 결정을 인터페이스 구현체에게 맡기는 유연하고 확장적인 구조이며 이로 인해 추출된 데이터들을 여러 방식으로 Load할 수 있다는 장점이 존재하지만, 제가 구현해야 할 작은 크기의 파이프라인에서는 S3 외에 당장 구현할 필요도 없습니다.

더해서 데이터 삽입 시 부과되는 S3의 요금을 생각하면 `Loader` 내부의 배치 처리가 강제되고, 그렇지 않으면 요금 폭탄을 맞게 되는 이지선다에 놓이게 됩니다.

### 결과

S3를 데이터 레이크로 결정한 이상 list 형태로 `Loader`를 구현하는 것으로 합니다. 하지만 이 방식에도 풀리지 않은 문제가 없는 것은 아닌데요. 예를 들어 "메모리의 압박은 어떻게 할 것인가?" 가 남아있습니다.

이에 대한 결론은 '"Orchestrator"의 책임으로 구분하는 것이 좋다'입니다. `Loader`의 책임은 배치를 파일로 변환하는 것입니다. 그리고 `Orchestrator`의 책임은 스트림을 배치로 만드는 것이죠. 만약 메모리에서 처리할 수 있는 배치의 크기를 `Orchestrator`에서 제어할 수 있다면 이 문제는 해결할 수 있습니다 예를 들어 다음과 같이 설계합니다.

```python
async def run_pipeline():
    ...
    batch = []
    BATCH_SIZE = 1000  # <-- 메모리 한계를 1000개로 제어

    # 파이프라인 시작
    async for item in extractor.extract():
        batch.append(item)

        # 배치가 차면 Loader 호출
        if len(batch) >= BATCH_SIZE:
            key = f"raw/games/{uuid.uuid4()}.jsonl"
            await loader.load(batch, key) # <-- list(배치) 전달
            batch.clear() # 메모리 해제

    # 만약 배치가 남았다면
    if batch:
        key = f"raw/games/{uuid.uuid4()}.jsonl"
        await loader.load(batch, key)

    # 파이프라인 종료
```

이러면 메모리의 압박에서 벗어남은 물론, 다른 부가 처리 없이 `extractor`만으로 파이프라인이 스트리밍으로 동작하게 됩니다.

## [CI] S3 통합 테스트 CI 환경 `AccessDenied`

2025-11-04

### 문제

> [!WARNING]
> Github Actions에서의 S3 접속 에러가 생긴다

ELT 파이프라인의 "L"의 기초 구현을 마무리하고, 통합 테스트를 작성하였습니다. 단위 테스트와는 다르게 S3 상태에 의존하고 있기 때문에, 환경 변수를 비롯해 꼼꼼한 설정이 필요합니다. 이때 테스트 작성 흐름은 다움과 같습니다.

- S3 세션 생성하기
  - 비동기 세션 사용 (~~boto3~~ -> **aioboto3**)
- S3 버킷, AWS 액세스 키, AWS 액세스 시크릿 불러오기
  - 이 단계에서 이미 AWS 서비스에서 IAM을 생성하고, 이 파이프라인 전용 버킷을 만든 뒤 최소 권한을 설정해주어야 합니다.
- 새로운 `jsonl` 테스트 파일을 생성하고, 테스트 데이터를 작성한 뒤 `S3Loader`로 실제 S3 세션에 접속해 업로드합니다.
- 업로드 후 `response`를 읽어 실제로 데이터가 S3 버킷에 쓰여졌는지 확인합니다.
- 모든 테스트가 종료되면 업로드된 파일을 삭제합니다.

여기서는 비동기 세션, `yield`, try-finally 패턴 등 비동기적인 패턴을 안전하게 처리하기 위한 코드가 많았습니다.

통합 테스트를 로컬에서 성공한 것을 확인하고 Github Secret에 환경 변수를 설정 후, Github Actions에 해당 브랜치를 머지하기 하기 위해 PR을 만들었습니다. PR 단계에서 실행된 CI 파이프라인에서 실패가 발생했고, 로그를 확인해보니 `AccessDenied` 에러를 반환하였습니다.

권한 거부 에러는 보통 AWS IAM에서 발생하는 경우가 많기 때문에 이 부분을 확인했으나, 로컬과 같은 IAM Key를 사용했고 로컬에서는 아무 문제가 없었기 때문에 다른 문제가 원인이 되었을 가능성이 높았습니다.

### 해결

첫 번째로 CI 로그에 디버깅 단계를 추가하고, `Arn`을 로컬과 비교했으나 모두 일치했습니다. 이는 인증이 성공했음을 의미합니다.

두 번째로 S3 버킷의 이름을 확인하였습니다. Github Secret을 다시 재수정하고, CI 로그에서 버킷 이름을 출력하는 디버깅 단계를 추가하였습니다. 로그에 `***`으로 마스킹되었고, 이는 올바르게 Secret을 로드하고 있음을 의미합니다.

마지막으로 환경 변수를 비롯해 TDD 코드와 IAM 정책에 아무런 문제가 없음을 증명하고자 Actions에서 직접 `awscli`를 이용해 S3에 파일을 직접 올리고 이를 CI 로그에 디버깅하는 테스트를 진행하였습니다. 이 디버깅이 성공하였고 결국 문제는 코드의 `aioboto3`에 있었습니다.

결과적으로는 `aioboto3`의 `region_name`을 명시적으로 주입하지 않아 버킷을 찾지 못하는 것이었고, 이를 아래와 같이 수정하니 해결되었습니다.

```python
@pytest.fixture(scope="function")
async def s3_client():
    """실제 aioboto3 S3 클라이언트 세션을 생성합니다."""
    region = os.getenv("AWS_DEFAULT_REGION")

    session = aioboto3.Session(region_name=region)
    async with session.client("s3", region_name=region) as client:
        yield client
```

이 이후에도 다른 문제로 AWS의 최종 일관성 문제가 발생시키는 간헐적인 오류가 발생하였습니다. 이는 `Gemini`에 따르면 `PutObject`후 바로 `Getobject`를 시도하여 (최종 일관성 모델을 따르는 S3에서는) 간헐적으로 실패가 생기고, 이때 AWS SDK에서 해당 파일이 있는지 확인하기 위해 내부적으로 `ListBucket`을 시도하다가 권한 문제로 실패하면서 생기는 에러였습니다. IAM 권한에 `ListBucket`권한을 해당 버킷에 추가하여 최종 일관성 문제를 해결하였습니다.

### 결과

IAM 정책에 `s3:ListBucket` 권한을 추가하여 CI의 Flaky Test(간헐적 실패)를 최종적으로 해결하였습니다.

근본적인 원인은 `aioboto3`에서 region_name을 채워주지 않아서였습니다. 거기에 까다로운 에러까지 겹쳐 평소에는 할 일이 없기도 하고 디버깅하기도 까다로운 CI 파이프라인을 디버깅하는 방법에 대해서 조금이나마 찾아볼 수 있었습니다.

더해서 S3의 최종 일관성에 대해 알아볼 수 있었습니다. S3 노드 간에는 데이터 전파 지연이 생기고 데이터를 업로드하는 즉시 확인하면 Not Found가 생길 수도 있다는 점, 그리고 그 후에 AWS에서 자동적으로 ListBucket을 시도한다는 점도 흥미로웠습니다.

현재는 Loader를 마무리하고 `EL` 두 단계를 E2E 테스트를 통해 검증하고, T로 넘어가도록 하겠습니다.

## [Design] TDD 사이클과 리팩토링

2025-11-05

### 문제

> [!WARNING]
> TDD 사이클에서 리팩토링의 적절한 기준이란?

TDD 사이클에 따라 기본적인 `EL`을 E2E 테스트까지 진행한 후의 다음 작업은 N+1 문제를 해결하는 것이었습니다.

IGDB API의 game 엔드포인트 응답은 거의 모든 item 항목이 정수로 되어 있습니다. 아마 모든 데이터가 정규화 원칙에 따라 잘 구조화되어 있는 것이겠지요. 하지만 모든 데이터를 추출해야 하는 저는 결국 game 테이블 외에 다른 모든 테이블들을 추출해야 하는 입장에 놓이고 말았습니다. 결국 게임 데이터 1개를 가져오기 위해서는 다른 테이블들(N개)를 참조해야만 하는 N+1 문제가 생긴 것입니다.

저는 platform, genre 등 주요 테이블에 대한 Extractor를 추가하는 것을 시작으로 했습니다. 모든 테이블을 서둘러 추가하지 않는 이유는 결국 이 파이프라인의 핵심은 파이프라인의 구조적인 비즈니스 로직이지, 테이블 하나 하나가 아니기 때문입니다.

같은 방식으로 테스트 코드를 작성한 뒤, 모든 테스트를 통과하는 Extractor를 만들었지만 이제 여기서 중복되는 코드가 생기기 시작합니다. 관측 가능성을 높이는 로그 시스템을 넣기 적합한 타이밍(현재의 코드 베이스가 아직은 적기 때문에)이기도 하기 때문에 기능 개발이 아닌 이런 모든 리팩토링을 TDD 개발 사이클에서 언제 진행해야 하는지 고민이 되었습니다.

### 해결

TDD에서 `REFACTOR` 단계는 다음과 같이 정의합니다.

> 새로운 기능 추가 없이, 테스트가 GREEN을 유지하는 상태에서 코드 구조를 개선하는 것.

이 원칙에 따르면 기능을 추가하거나 수정하는 것이 아닌 중복 코드를 묶어주는 행위는 `REFACTOR` 단계에서 진행할 수 있습니다. 로그 또한 테스트를 변경해야 하는 기능 추가 단위의 코드 수정이 아니기 때문에 이 단계에서 진행하는 것이 옳겠죠.

게다가 중복되는 코드의 발생은 오히려 좋은 징조일 수 있습니다. 이 후에 추가할 테이블들을 추출하는 Extractor들은 대부분 같은 코드를 공유하게 될 것이기 때문입니다. 게다가 로깅을 비롯해 Retry(재시도) 로직 등, Extractor 간에 공통적으로 가져가야 할 로직이나 라이브러리 추가와 같은 코드 수정을 하나의 코드로 통일한다면 훨씬 효율적으로 구조화할 수 있습니다.

따라서 저는 이 브랜치(`feature/platform-extractor`)에서 `BaseIgdbExtractor` 클래스를 생성해 모든 공통 로직을 포함하고, 달라지는 부분 (엔드포인트, 응답 개수) 등을 클래스 변수로 정의하도록 리팩토링했습니다. 달라진 코드는 다음과 같습니다.

```python
# src/pipeline/extractors.py

# (REFACTOR) 공통 로직이 Base 클래스로 이동
class BaseIgdbExtractor(Extractor):
    _API_URL: str
    _BASE_QUERY: str = "fields *;"
    _LIMIT: int = 500

    def __init__(...):
        # (공통 __init__)

    async def extract(...):
        # (공통 페이징, 인증, 로깅, 에러 핸들링 로직)

# (REFACTOR) 자식 클래스들은 설정만 남김
class IgdbExtractor(BaseIgdbExtractor):
    _API_URL = "[https://api.igdb.com/v4/games](https://api.igdb.com/v4/games)"
    _LIMIT = 500

class IgdbPlatformExtractor(BaseIgdbExtractor):
    _API_URL = "[https://api.igdb.com/v4/platforms](https://api.igdb.com/v4/platforms)"
    _LIMIT = 50
```

### 결과

리팩토링 후 `uv run pytest`를 실행했을 때, 모든 테스트가 [GREEN]임을 확인하면 완료입니다.

다음 차원을 추가하는 작업 또한 굉장히 간단해졌고, 테스트 코드 또한 해당 원칙을 이용해 중복 코드를 생략하는 방식으로 접근할 수 있습니다. 다만 테스트 코드의 경우, 테스트의 목적인 명확성을 위해 테스트 과정이 아닌 `conftest.py`의 fixture를 통해 데스트 셋업의 중복 코드를 줄이는 것을 목표로 합니다.

TDD에서 리팩토링은 수시로 해줘야 하는 것 같습니다. 최소 기능 단위로 개발하고, 이를 테스트로 검증하면서 수시로 리팩토링하는 접근법 또한 실용성이 돋보이는 것 같습니다.

## [Design] 추상 클래스로의 접근 시점

2025-11-05

> [!WARNING]
> 쓰지 말아야 하는 `hasattr`, 그리고 써야하는 `@abstractmethod`, `@property`의 차이

아까 리팩토링을 통해 IGDB 공용 Extractor 클래스를 사용하면 다음과 같이 손쉽게 새로운 Extractor를 만들 수 있었습니다.

```python
class SomeExtractor(BaseIgdbExtractor):
    _API_URL = "..."
    _BASE_QUERY = "fields *;"
    _LIMIT = 500
```

그리고 `BaseIgdbExtractor` 클래스에서는 이런 부분이 존재합니다.

```python
if (
    not hasattr(self, "_API_URL")
    or not hasattr(self, "_BASE_QUERY")
    or not hasattr(self, "_LIMIT")
):
    raise NotImplementedError(
        "서브클래스에서 _API_URL, _BASE_QUERY, _LIMIT 속성을 정의해야 합니다."
    )
```

이 코드는 왜 위험할 수 있을까요?

추상 클래스는 단순하게 표현하자면 구현의 형식을 강제하기 위해 만든 클래스라고 할 수 있습니다. 하지만 만약 위와 같이 `hasattr`(속성을 가지고 있는가?)만을 통해 속성을 검사하게 된다면, 코드가 실행되고 `__init__` 메서드가 호출되어야만 `NotImplementedError` 오류를 발견할 수 있습니다. 쉽게 말해 선언 당시에는 오류를 발견할 수 없다는 의미이지요.

심지어 `_API_URL`이 `None`이라고 설정되어도 검사는 통과하지만 실제 코드는 실패하는 경우가 생길 수도 있습니다.

### 해결

이를 해결하기 위해 해결책으로 제시된 방법이 `@abstractmethod`와 `@property`입니다. 이를 통해 추상 속성을 만들 수 있습니다.

먼저 클래스가 `ABC`를 상속받도록 하고, 내부의 속성 설정을 메서드처럼 정의합니다. 코드는 다음과 같습니다.

```python
from abc import ABC, abstractmethod

class BaseApiExtractor(Extractor, ABC):
    """
    API 기반 Extractor를 위한 추상 기본 클래스.
    """

    @property
    @abstractmethod
    def _API_URL(self) -> str:
        """API 엔드포인트 URL"""
        pass

    @property
    @abstractmethod
    def _BASE_QUERY(self) -> str:
        """API에 보낼 기본 쿼리"""
        pass

    @property
    @abstractmethod
    def _LIMIT(self) -> int:
        """페이지네이션 등을 위한 API 호출 제한"""
        pass
```

이렇게 작성한다면 세 가지 속성 중 하나라도 빠뜨릴 시, 인스턴스를 만드는 순간 `TypeError`가 발생합니다. 정적 분석 시점에서 오류가 발생하게 되는 것입니다. 빠르게 오류를 발생시켜 테스트의 이유를 명확하게 정의할 수 있습니다.

추가로 `NotImplementedError`가 아닌 Python 표준 오류인 `TypeError`를 발생시킨다는 점도 더 안정적이라고 생각할 수 있습니다.

### 결과

`BaseIgdbExtractor`는 위와 같이 변경하였고, 이를 상속하는 클래스는 다음과 같이 선언할 수 있습니다.

```python
class IgdbPlatformExtractor(BaseIgdbExtractor):
    """IGDB API로부터 플랫폼 데이터를 추출하는 Extractor 구현체."""

    @property
    def api_url(self) -> str:
        return "https://api.igdb.com/v4/platforms"

    @property
    def base_query(self) -> str:
        return "fields *;"

    @property
    def limit(self) -> int:
        return 50
```

추가로 속성이 `_API_URL`과 같이 대문자로 표기되면 린트 오류가 발생되기 때문에, 모두 소문자로 바꾸는 작업을 추가로 진행하였습니다.

## [Architecture] 어떤 차원 데이터를 가져와야 할까?

2025-11-05

### 문제

> [!WARNING]
> 어떤 IGDB 데이터를 우선적으로 가져와야 할까?

위에서도 언급했던 문제이지만 IGDB API는 games 외에도 많은 차원 테이블을 제공합니다.

- platforms, genres, game_modes (핵심 메타데이터)
- themes, player_perspectives (부가 정보)
- companies, franchises (비즈니스 정보)
- artworks, screenshots, videos (미디어)
- age_ratings, websites, release_dates (세부 정보)
- ... 총 40개 이상의 엔드포인트

모든 데이터를 추출하면 완벽하겠지만, 작업은 순차적으로 진행되기 때문에, MVP 관점에서 우선 순위를 정해야 했습니다.

### 해결

저는 다음과 같은 기준으로 차원 데이터의 우선 순위를 결정하였습니다.

**게임 분류의 핵심:**

- `platforms` - "어떤 플랫폼에서 나왔는가?"
- `genres` - "어떤 장르인가?"
- `game_modes` - "싱글플레이? 멀티플레이?"
- `themes` - "게임의 주제/분위기"
- `player_perspectives` - "1인칭? 3인칭?"

**차후 확장 시 추가:**

- `companies` - 개발사/퍼블리셔 정보
- `franchises` - 시리즈 정보
- `release_dates` - 출시일 상세 정보

일단은 분석에 용이한 데이터들을 먼저 확보한 뒤, 데이터들의 분석 가치를 만들고 다른 데이터들을 가져오는 전략을 택하였습니다.

### 결과

현재 차원 테이블 5개를 모두 구현하였고, 파이프라인을 완성한 후 필요에 따라 점진적인 확장을 진행합니다.

제가 목표로 삼은 소프트웨어가 작게 시작해서 테스트로 검증하며 점진적으로 확장하는 방식이기 때문에 이렇게 결정했습니다.

다음 단계로 Transform을 구현하도록 하겠습니다.

## [Architecture] `dbt` 초기 과정

2025-11-07

> [!WARNING]
> 처음 `dbt`를 접했을 때

### 문제 & 해결

`dbt`를 처음 접하면 사실 문제가 뭔지도 파악할 수 없는 상황에 놓일 가능성이 높습니다. 저 같은 경우에는 엔터프라이즈급 데이터 파이프라인에서나, 경량 파이프라인에 Transform에 표준적으로 쓰이는 기술 스택이라고 언급되었기 때문에 사용해보자고 생각했고, 실제 Transform 과정을 Python 스크립트로 짰을 때 엄청나게 고통을 받았던 경험이 있기 때문에 이 문제를 어떻게 효율적으로 풀어 나갔는지도 궁금했습니다. (자세한 이유는 [참고](./02_Tech_Stacks.md#dbt))

다만 개인적인 인상으로는 `dbt`가 어떤 구조를 통해 데이터 변환 과정을 구조화하려고 했는지는 알겠으나 처음 접할 때의 Transform 레이어 구성을 보면 직관적이라는 단어와는 거리가 멀다고 할 수 있습니다.

큰 그림을 봤을 때, 제가 파악한 `dbt`의 초기 작업 과정은 다음과 같습니다.

1. 프로젝트 내에서 `dbt-<adapter>`(프로젝트에서는 `DuckDB` 사용) 의존성을 설치하고 초기화
2. 환경 설정 - `dbt_project.yml`과 `~/.dbt/profiles.yml`, `models/schema.yml`등을 통해 로컬/S3 환경 분리나 테스트할 컬럼의 구조 등을 설정
3. Mock 데이터나 프로젝트 내의 `/seeds`를 사용한 테스트를 통해 데이터 구조를 검증
4. 실제 데이터를 연결하는 작업을 `SQL`과 `Jinja`를 통해 작성한다.
5. (테스트 환경과 실제 환경을 분리했다면)`dbt run --target [ENV]`로 데이터 웨어하우스에 모델을 빌드하고, `dbt test --target [ENV]`로 결과물을 테스트

저는 환경을 두 가지로 나누었습니다. 개발과 테스트 과정에서도 매번 S3에서 진행할 수는 없었기 때문입니다.

- `dev_local_tdd`
- `prod_s3`

다음은 초기 작업 과정 중 생긴 문제들입니다.

첫 번째로 환경 설정 과정에서 `profiles.yml`의 위치가 어디에 존재해야 하는지에 대한 혼선이 있었습니다. 결과적으로 `profiles.yml`는 사용자 개인의 홈 디렉토리 `~/.dbt/`에 존재해야 합니다. 이는 보안과 이식성을 고려한 것으로 git에서 관리하지 않는 파일입니다.

두 번째로 `attach` 설정의 존재 위치입니다. 해당 설정은 실제 목 데이터를 바탕으로 테스트 데이터를 만들기 위해 `dev_local_tdd` 환경에 생성하려고 했으나 해당 설정에 대한 문법에 대해 확실하지 않아 여러 가지로 시도해 본 결과, `attach` 설정을 포기하고 아래와 같이 직접 SQL에서 `read_json_auto()` 함수를 사용하는 것으로 결정했습니다.

```sql
{% if target.name == 'dev_local_tdd' %}
SELECT id, name
FROM read_json_auto('seeds/igdb_games_mock.jsonl')
```

해당 소스 코드는 Jinja 템플릿으로 환경별 데이터 소스를 분기하고, DuckDB에서 네이티브로 지원하는 `read_json_auto()`를 통해 json파일을 자동을 읽게 했습니다.

### 결과

지금까지 완료한 `dbt`의 실행 흐름을 요약하자면 다음과 같습니다.

```plain
1. 사용자 명령
   └─> uv run dbt run --target dev_local_tdd

2. dbt 초기화
   ├─> profiles.yml 읽기
   │   └─> dev_local_tdd 타겟 선택
   │       └─> type: duckdb, path: tdd_warehouse.db
   │
   └─> dbt_project.yml 읽기
       └─> models/staging/*.sql 찾기

3. 모델 컴파일
   ├─> stg_games.sql 읽기
   ├─> Jinja 템플릿 처리
   │   └─> target.name == 'dev_local_tdd' → True
   │       └─> read_json_auto('seeds/igdb_games_mock.jsonl')
   │
   └─> 최종 SQL 생성
       └─> target/compiled/transform/models/staging/stg_games.sql

4. SQL 실행
   ├─> DuckDB 연결: tdd_warehouse.db
   ├─> CREATE VIEW main.stg_games AS
   │   └─> SELECT id, name FROM read_json_auto('seeds/...')
   │
   └─> DuckDB가 JSONL 파일 읽기
       └─> 스키마 자동 감지
           └─> VIEW 생성 완료

5. 결과 출력
   └─> ✅ Done. PASS=1 WARN=0 ERROR=0
```

그리고 `attach` 삽질을 통해 깨달은 두 설정 파일의 역할은 다음과 같습니다.

**profiles.yml (Connection):**

- WHERE: 어디에 연결할지
- HOW: 어떻게 인증할지
- WHAT: 어떤 환경인지

```yaml
outputs:
  dev_local_tdd:
    type: duckdb
    path: tdd_warehouse.db # 어디에 저장할지
```

**dbt_project.yml (Configuration):**

- WHAT: 무엇을 빌드할지
- HOW: 어떻게 빌드할지 (materialization)
- WHERE: 모델 파일이 어디 있는지

```yaml
models:
  transform:
    staging:
      +materialized: view # 어떻게 빌드할지
```

이 후 SQL을 통해 DuckDB가 성공적으로 `jsonl` 파일을 파싱하고, View를 생성한 것을 확인한 후, EL 작업이 연결된 다른 차원 테이블의 응답 `json` 파일을 실제로 변환해 같은 구조로 정리하였습니다.

여기서 그치지 않고 bridge sql을 통해 차원 테이블과 팩트 테이블(`games`)의 값을 연결하는 테스트와 실제 View를 생성해 데이터 마트 작업에 대한 준비를 마쳤습니다.

## [Architecture] GitHub Actions vs. Airflow

2025-11-07

> [!WARNING]
> 오케스트레이션으로 어떤 것을 골라야 할까?

### 문제

오케스트레이션은 실제 ELT 파이프라인을 프로덕션 환경에서 구동시키는 역할을 합니다. 문제는 제가 이 결과물들을 어떻게 보존하고, 또 어떤 목표로 나아가야 할 지에 따라 이 오케스트레이션의 기술 스택 선택 기준이 크게 달라진다는 점입니다.

실제로 `Airflow`는 실무 환경에서 압도적으로 많이 쓰이지만, 제가 구축하려는 가벼움과는 거리가 멀고, 심지어 24시간 서버 가동이 강제화되며, 설정에서 상당히 많은 시간을 소모할 가능성이 높습니다.

그렇다고 서버리스로 구축한다면, 실시간성을 잃어버리는 것은 파이프라인이라는 비즈니스 제품에서는 매력을 크게 잃어버리는 것과 같습니다. 비용 효율적이라는 면이 있지만 실제 가동되는 파이프라인과는 거리가 멀겠죠. IGDB의 웹훅을 유용하게 활용하지도 못 할 것이구요.

여러 방안을 검토해 본 결과...

### [해결](./02_Tech_Stacks.md#github-actions-vs-airflow-or-dagster)

ELT 로직이 완성되고 나서, 비용 효율적인 오케스트레이션을 위해 서버리스인 Github Actions 워크플로우로 구축하기로 결정하였습니다. needs 키워드를 사용해 EL(Python) 작업이 성공하면 T(dbt) 작업이 prod_s3 타겟으로 실행되도록 E2E 파이프라인을 자동화하는 것입니다.

이 방식은 CI/CD와 통합되고 `Github`에서 실행된다는 점 때문에 비용이 0이라는 강력한 장점이 있지만 전문 오케스트레이터에 비해 기능이 부실하고, 데이터 중심의 UI가 없다는 트레이드오프가 존재합니다. 따라서 포트폴리오의 로드맵을 투트랙으로 진행해 실제 로직이 모두 완성되고 구현이 확인되면 마이그레이션을 진행하고, `Airflow`나 `Dagster`로 구현을 진행할 예정입니다.

이와 같은 결정의 자세한 내용과 결과는 [Tech Stacks 문서](./02_Tech_Stacks.md#github-actions-vs-airflow-or-dagster)를 확인해주세요.

## [Library] `dbt`에서 일반적으로 git에 업로드하는 파일들은?

2025-11-08

### 문제

> [!WARNING]
> Git에 저장되어야 하는 `dbt` 파일을 알아보자

`dbt`의 프로젝트 구조는 직관적이지 않다고 아까 말씀드렸죠. 사실 사용하다보면 이 방법이 데이터 변환 과정을 그나마 최대한 직관적으로 만들어 놓은 방법이라는 생각이 들기도 합니다. 역시 데이터마다 너무나 다른 변환이라는 과정 자체를 계층으로 만드는 것은 쉽지 않다는 생각이 드네요.

덕분에 구조를 어느 정도 파악하고 난 뒤 `dbt`의 워크 플로우를 올바르게 저의하고 나면 프로젝트를 버저닝(Versioning)하기 위해 어떤 파일이 저장소에 공유되어야 하고, 어떤 파일들이 공유되지 않아야 하는지가 궁금해졌습니다.

### 해결

잘 생각해보면 `dbt`에서 하는 일은 명확합니다. 주어진 정보를 바탕으로 데이터를 처리하는 방법을 기억하는 것이죠. 데이터 간 의존성 관계에 따라 데이터를 연결하는 방법, 컬럼의 값들을 테스팅하는 방법, 그리고 이 변환을 이름붙이고 계층화시켜 원하는 데이터의 형태를 만드는 순서와 방법을 모두 기억하는 것입니다. 그렇다면 Git에 공유되어야 하는 건 결과물(순서와 방법)이 아닌 **이 결과물을 만드는 로직**일 것입니다.

따라서 `dbt` 프로젝트의 로직과 테스트를 정의하는 모든 파일인 `models/`, `seeds/`, `tests/`, `dbt_project.yml`, `packages.yml`을 필수적으로 포함해야 합니다.

그리고 제외해야 하는 파일은 `dbt run`으로 생성되는 모든 빌드 파일(`target/`), `dbt_packages/`, `logs/`, DB 파일 등입니다. `profiles.yml` 또한 개인 정보를 포함하고 있으므로 개인 루트 폴더에 보관하고 관리해야 합니다.

### 결과

결국 공유 저장소에서의 본질은 비슷합니다. 우리가 `.gitignore`에 넣어야 하고, 넣지 말아야 하는 파일을 구분할 때, 산출물들을 저장하지는 않죠.(설치된 패키지-`node_modules`폴더 등과 같은) 대신 그 산출물들을 정의하는 파일을 공유합니다.(`package.json`과 같은) 비슷한 논리에 의해 `dbt`에 들어가는 파일과 폴더들을 구분한다면 그렇게 어렵지만은 않은 문제였습니다.

## [CI] 오케스트레이션을 정의하기 이전에 확인해야 할 것은 뭘까?

2025-11-08

> [!WARNING]
> E2E 테스트 등 파이프라인이 실제 가동되는지 확인해야 하지 않을까?

### 문제

비용 문제를 비롯한 문제들을 고려했을 떄, 이것 저것 라이브 데모를 구축하기 위한 방안으로 결국 CI 파이프라인을 통해 주기적으로 데이터 파이프라인을 가동시키는 방식이 좋겠다고 결론을 내린 뒤, 최소 단위의 파이프라인 구축이 끝났습니다.

하지만 아무리 테스트 코드를 통해 많은 오류를 잡았다고 하지만, 프로덕션과 가까운 레벨에서 생길 수 있는 버그에 대한 것은 아무리 경력이 많은 개발자라도 예상하기 어려운 지점일 것입니다.

저는 오케스트레이션을 위한 브랜치를 파고 나서야 이 생각이 들었는데요. 현재 통합 테스트나 E2E 테스트는 EL 파이프라인에 한해서만 하고 있었고, T는 Actions의 CI에서 진행하고 있지 않은 만큼, 둘이 합쳐졌을 때 어떤 오류가 나기 전에 테스트를 진행해야겠다고 생각했습니다. 파이프라인의 프로토타입이 완성된 지금이 최적의 타이밍이기도 한 것이죠.

### 해결

결국 ELT 전체를 엮어 최종 E2E 테스트를 직접 진행해보고, 특별한 오류가 발생하지 않으면 오케스트레이션용 CI 스크립트를 작성하기로 하였습니다.

먼저 환경 변수를 모두 포함했는지 `.env`를 확인하고, 변수 명에 유의해서 실제 코드에서 불러오는 로직에 문제는 없는지 확인합니다. `run_pipeline.py` 스크립트-실제 파이프라인에서도 가동될-를 작성합니다. 이 스크립트는 EL 단계를 포함하며, 실제 IGDB에 존재하는 모든 필요한 데이터를 추출하여 S3에 적재합니다. 유닛 테스트와는 달리 실제 응답을 저장하므로, S3 콘솔을 통해 실제로도 확인하며 진행할 수 있습니다.

다음은 T, `dbt`스크립트를 실행하고 데이터 마트를 생성합니다. 앞에서 잘 정의되었다면 간단한 `dbt`명령어로 완료할 수 있습니다. 검증 단계는 test 명령어나 docs 명령어를 통해 리니지 그래프 등 시각화도 포함할 수 있습니다. 여기까지 정상적으로 수행된다면 모든 단계가 정상적인 상황에서는 오류가 나올 가능성은 없다고 생각합니다. 말 그대로 엔드 투 엔드 테스트 입니다.

### 결과

ELT를 모두 완료하고 나서야 E2E를 확인할 수 있다는 사실은 작은 사이즈의 소프트웨어들을 만들어온 저에게는 아무래도 아직까진 생소한 부분이 아닐 수 없습니다. 그리고 이 단계에서 테스트 코드들의 효율성이 다시 체감될 수 밖에 없는데, 필요한 최소한의 테스트 코드만으로도 전체 과정을 여러번 반복하지 않으면서 잘 작동되는 하나의 파이프라인을 구축할 수 있다는 점은 개발 효율성에서 TDD의 강점을 잘 보여주는 사례인 것 같네요.

## [dbt] `dbt` 환경 변수 설정은 어떻게 할까?

2025-11-08

> [!WARNING]
> 과연 `dbt`는 환경 변수를 읽을 수 있을까?

### 문제

현재 `dbt`의 개념부터 실전까지 완전히 바닥부터 시작하고 있기 때문에, 설정 또한 하나 하나 차례로 배워가면서 진행중입니다.

다만 제가 하나 의문인게 있었다면, `dbt`는 현재 S3를 사용하고, 이후에도 AWS의 여러 서비스를 사용할 수 있는 상황에서 환경 변수 사용이 강제되고 있습니다. 아마 대부분의 프로젝트도 그럴 것이라고 생각합니다. 하지만 제 프로젝트의 경우 `dbt` 프로젝트 디렉토리는 전체 프로젝트 내부에 존재하며 종속되어 있지만, `dbt` 프로젝트에서는 애초에 상위 프로젝트의 일부로 인식하고 있지 않은 것 같고, 실제로도 전체 프로젝트의 환경 변수를 바로 설정 YAML 파일에 불러올 수 있는 방법이 아직까지는 표준처럼 존재하지는 않는 것 같습니다. 설정으로 같이 쓰면 정말 쉽고 좋을 것 같은데요.

### 해결

찾아본 결과 많은 해결 방법에서 `EXPORT`를 통해 환경 변수를 cli에서 수동으로 주입하는 방법이 계속해서 제시가 되었고, 저는 테스트나 벤치마크를 비롯해 반복해서 이 방식을 수행해야 하는 입장에서 솔직히 납득이 가는 솔루션은 아니였습니다. 왜 프로젝트에 환경 변수가 존재하는데 이걸 다시 설정해주어야 하는지도 이해가 잘 안갔고요.

결국 다른 방법이 있다면, 외부에서 env 파일을 찾아 cli 수준에서 주입해주는 동일한 방식이지만 플래그를 통해 하나의 문장에서 실행시키도록 하는 방법을 사용하기로 하였습니다. 그나마 하나의 명령어 수준에서 진행될 수 있는 가장 깔끔한 방법인 것 같습니다.

명령어는 다음과 같습니다.

```bash
uv run --env-file ../.env dbt test --target prod_s3
```

까고 본다면 `uv` 수준에서 프로젝트 외부에 있는 `.env` 파일을 `dbt`에 주입해주는 동일한 방식이지만 그냥 쓰기로 했습니다. 분명히 다른 방법이 있는 것 같은데 잘 모르겠어요. `dbt Cloud`를 쓰는 방식도 있는 것 같은데 그 부분은 클라우드 데이터베이스 시스템에 연동을 해야 하는 것 같아서 그만두기로 했습니다.

### 결과

영 찜찜하게 해결하긴 했지만, 결과적으로 테스트나 실행이 문제없이 진행된다면 이 방법을 쓰는 것이 최선인 것 같습니다. 다만 이 후에 `GitHub Actions`나 타 오케스트레이션 시스템에서 사용하는 방식을 추가해야 하는 상황에서 이 방법을 어떻게 연결할 수 있을지, 실제로 어떤 흐름으로 실행되는지도 유의해야 할 점으로 보입니다.

## [Testing] Transform 후 중복 데이터 발견

2025-11-09

> [WARNING!]
> Transform 단계에서 문제가 생겼을 떄의 디버깅 방법

### 문제

`uv run --env-file ../.env dbt test --target prod_s3`를 통해 프로덕션 데이터베이스를 테스팅했고, 다음과 같은 문제가 발견되었습니다.

```log
10:25:10  Completed with 2 errors, 0 partial successes, and 0 warnings:
10:25:10
10:25:10  Failure in test unique_dim_games_game_id (models\marts\schema.yml)
10:25:10
10:25:10    compiled code at target\compiled\transform\models\marts\schema.yml\unique_dim_games_game_id.sql
10:25:10
10:25:10  Failure in test unique_stg_games_id (models\staging\schema.yml)
10:25:10    Got 3 results, configured to fail if != 0
10:25:10
10:25:10    compiled code at target\compiled\transform\models\staging\schema.yml\unique_stg_games_id.sql
10:25:10
10:25:10  Done. PASS=39 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=41
```

요약하자면 스테이징 단계에서 생성한 game 테이블에서 진행한 id 유일값 테스트가 실패했다는 의미입니다.

스크립트를 통해 문제의 ID를 발견해보니 중복값 3개가 발견되었고, 추정컨데 API에서 페이징을 통해 데이터를 수집하던 중, 데이터가 추가되거나 삭제되는 변경 등의 이유로 밀리게 되어 페이징이 제대로 작동하지 않았고, 그 결과 응답에서 중복값이 발생한 것으로 보입니다. 근거는 game 테이블의 중복 결과 값이 checksum이 같다는 것으로, 실제 완전히 동일한 데이터가 2번 저장된 결과임을 알 수 있습니다.

### 해결

문제 상황의 진단이 맞다는 가정 하에, 해결 방법으로는 다음 3가지를 생각할 수 있습니다.

**1. DISTINCT**

가장 간단한 방법으로 SQL에 DISTINCT를 추가하는 것을 생각해 볼 수 있습니다. 즉시 적용이 가능하며, 테스트도 최소한의 변경으로 통과할 수 있습니다. 뿐만 아니라 산출물인 `dim_games`도 중복이 제거됩니다 (팩트 테이블인 games에서 중복 id를 제거하므로). 하지만 이 방법은 다른 문제를 발생시킵니다.

1. E의 문제가 T에서 해결

   E단계의 `Extractor`에서 오염된 데이터를 만들었으나, T단계의 SQL에서 해결한다는 방식은 파이프라인의 책임 분리 원칙 그 자체를 위배합니다. 여기서 `Extractor` 단계에서 원천적으로 오염을 차단해야 하고 이를 위해 해당 모듈을 수정해야 함을 알 수 있습니다.

2. 데이터 레이크 오염

   위의 문제와 비슷한 문제로, E단계의 데이터는 어떠한 가공도 없이 데이터 레이크에 저장되기 때문에 중복 데이터가 존재할 경우 결국 데이터 레이크에는 중복된 데이터가 저장되는 문제가 생깁니다. 협업이라면 더욱 큰 문제가 될 수도 있습니다.

3. 성능 저하

   `SELECT DISTINCT * ... FROM`은 모든 행의 모든 열을 비교하여 중복을 제거하는 연산이므로 매우 비쌉니다. 현재 데이터 세트의 경우 중복 결과는 3건이지만 34만개 행의 50개가 넘는 열을 모두 비교해야 하는 오버헤드가 발생합니다. 물론 체크섬 만을 확인하는 경우도 있지만 이 또한 이상적인 방법은 아닙니다.

위와 같은 이유로 해당 방법은 적절하지 않습니다.

**2. `Extractor`에서 중복 제거**

추출 과정에서 모은 모든 game 데이터의 id를 수집합니다. Python에서는 중복값을 자동으로 없애고, 해시 연산을 지원하는 `set`를 이용하는게 이상적입니다. 가령 다음과 같이 수정할 수 있습니다.

```python
class BaseIgdbExtractor(Extractor, ABC):
    def __init__(self, ...):
        self._seen_ids: set[int] = set()
        ...

    async def extract(self, ...):
      ...
      for item in response_data:
        item_id = item.get('id')
        if item_id in self._seen_ids: # 여기서 중복 체크
```

코드 로직에 어느 정도 수정이 필요하다는 점을 제외하면 위 방법에서 제기했던 모든 문제를 해결할 수 있습니다. 최대로 사용되는 메모리를 계산해 본다면 Python에서 정수 34만개를 `set()`에 저장하는데 필요한 메모리는 약 10MB도 안되므로 충분히 감수할 수 있는 비용입니다.

**3. 쿼리 수정**

대부분의 API 페이징은 정렬을 기반으로 작동하며, 여기서 생긴 오류는 수집 단계의 정렬 과정에서 변화가 생겼을지도 모른다는 불확실성에서 발생합니다.

그렇기 때문에 `offset`뿐만 아니라 `id`값을 기준으로 정렬할 수 있도록 수정한다면 데이터 세트 내의 중복을 안정적으로 최소화할 수 있을 것입니다. 다음과 같이 쿼리를 수정합니다.

```python
@property
def base_query(self) -> str:
    return "fields *; sort id asc;"
```

### 결과

저는 여러 장단점을 고려해 봤을때 **3번,** 쿼리를 수정하여 `id`순으로 정렬해 수집하는 방식을 사용했습니다. 동시에 `Extractor`의 대부분 클래스들이 `query`와 `limit`을 대부분의 상황에서 바꿀 필요가 없다는 것을 깨닫고 모든 `Extractor`의 기본 값을 설정해 위의 쿼리와, limit을 API 최대 크기인 500으로 통일하였습니다. 만약 이처럼 하위 클래스에서의 프로퍼티의 구현을 강제하고 싶지 않지만 값을 변경할 수 있는 기본 템플릿을 주고 싶다면 `@property`를 유지하고 `@abstactmethod`만 제거한다면 쉽게 구현이 가능합니다.

결국 현재 존재하는 `Extractor`들은 API URL만 다른 추출 클래스가 되었네요.

## [Test] 끔찍하게 느린 `dbt test`

2025-11-10

> [!WARNING]
> 대체 `dbt test`는 왜 이렇게 느린 것일까?

### 문제

현재 데이터 수집의 프로토타입은 ELT 파이프라인을 수동으로 실행시키고 초기 데이터셋을 성공적으로 확보함으로서 실행과 테스트가 모두 통과되었습니다. 오류 없이 일단 데이터가 쌓입니다.

하지만 프로토타입을 시각화하기 전에 해결해야 할 문제가 있다면 ELT 마지막 단계인 `dbt test`가 자그마치 거의 2시간 정도의 시간이 소모된다는 것입니다. 이는 현재 프로덕션 단계에서 가장 큰 병목이 될 가능성이 높습니다.

문제는 또 `dbt` 설정에 있었는데요. `dbt`는 `run`후에 `test`하는 작업 흐름을 가지고 있습니다. 제가 `run` 단계에서 최종 데이터인 `dim_games`를 `VIEW`로 생성했고, 뷰는 테이블이 아닌 JOIN의 방법을 설명하는 SQL 그 자체라는 점입니다. 따라서 `test` 단계에서는 수집하고 생성한 데이터 무결성 등을 검사하기 위해, VIEW를 테스트 단계마다 실행해야 합니다.

제가 설정해 놓은 테스트가 41개의 테스트 케이스므로 단순 계산 기준 VIEW에서 정의한 연산을 41번 실행한 것이 됩니다. 거기에 테이블이 늘어날 수록 테스트는 계속 늘어나기만 할텐데 지금보다 더 엄청난 크기의 병목이 되겠죠.

### 해결

해결책으로 간단한 방법이 존재했는데요. 바로 `dbt`의 Materialization(구체화) 설정을 변경하는 것입니다. 로컬 테스트는 데이터 세트가 100개 정도로 작기 때문에 모두 VIEW로 생성한 후 무결성을 확인하는 것이 무리가 없습니다. 하지만 데이터 세트가 커질수록 VIEW는 하나의 테스트 당, VIEW 연산을 한 번 씩 진행하며 재앙적인 속도로 변하게 되어, 프로덕션일 때는 테이블로 생성해 사전 연산으로 저장해 놓은 상태에서 테스트를 진행하는 것이 효율 면에서 압도적입니다. 예시는 다음과 같습니다.

```SQL
-- marts/dim_games.sql
{{
  config(
    materialized = 'view' if target.name == 'dev_local_tdd' else 'table'
  )
}}
```

이 방법을 제대로 쓰기 위해서는 비싼 연산과 값싼 연산을 구분해야 합니다. 실제 연산을 하지 않는 `SELECT`, `CAST`등 가벼운 작업에 대해서는 VIEW로 유지하고, Mart나 Bridge와 같은 `JOIN`, `UNNEST`같은 무거운 연산은 TABLE로 저장하는게 좋습니다.

### 결과

로그를 요약하자면 다음과 같습니다. 먼저 초기 ELT 과정에서 소요된 시간입니다. (실제 로그 아님)

```log
2025-11-10 11:38:10.112 | EL 작업 시작
2025-11-10 11:38:11.395 | games 엔티티
2025-11-10 11:52:51.056 | games 엔티티 완료
...
2025-11-10 11:53:00.112 | 데이터 추출 및 적재 완료 총 소요 시간: 890.00초
================================================
11:53:05.048428 | Transform 작업 시작
...
12:03:39.488399 | Transform 작업 완료
총 소요 시간: 634.44초
================================================
12:49:23.804901 | Transform 테이블 Test 시작
...
14:36:53.177622 | Test 완료
총 소요 시간: 6449.37초
```

ELT 총 작업 시간은 7973.81초로 2시간 13분 정도가 소요되었고, 그 중 Transform에 소요된 시간은 테스트를 합쳐 1시간 58분입니다.

다음은 dim_games과 bridge 변환의 Materialization 설정을 SQL단에서 뷰에서 테이블로 바꾸고 난 후의 로그입니다. Tranform 단계만 측정합니다.

```log
07:52:52  | Transform 작업 시작
...
08:18:10  | Tranform 작업 완료
총 소요 시간: 1516.73초
===============================================
08:18:15  | Transform 테이블 Test 시작
...
08:46:19  | Test 종료
총 소요 시간: 1682.87초
```

총 소요 시간은 3199.6초로 성능이 약 **55%** 개선된 것으로 확인됩니다. 데이터 세트가 작지는 않다 보니 테이블과 뷰의 차이 만으로 이 정도의 변화를 만들 수 있습니다. 이 후 최적화 과정과 변화는 [성능 문서](./04_Performance.md#tdbt)를 참고해주세요.

테이블화의 단점으로는 저장 비용의 증가와 데이터의 최신성에서 하자가 생긴다는 것인데요. 결국 데이터 정책은 분산 컴퓨팅에서도 마찬가지겠지만 최신성, 일관성 등의 키워드를 비즈니스 모델에 따라 얼마나 적절하게 타협할 수 있는가에 달려 있다고 보아야 할 것 같습니다. 게임 데이터의 경우에는 완벽한 최신성을 요구하지 않으며 실시간 업데이트 파이프라인으로 개발될 경우에도 IGDB에서 제공하는 웹훅을 통해 업데이트 간격을 동적 결정하는 방향으로 설계한다면 대부분의 요구 사항을 적절하게 충족할 것 같습니다.

## [Refactor] Orchestration을 위한 구현

2025-11-11

> [!WARNING]
> 반복적인 수집 작업을 최적화해보기

### 문제

현재의 파이프라인은 `Full Load` 방식으로, 실행 시에는 34만 건 이상의 API를 호출하고, 매일 데이터를 S3에 덮어쓰며, 25분의 `dbt run`을 실행해 데이터 마트를 통째로 재구축합니다. 만약 제가 이 파이프라인을 통해 달성하고 싶었던 목표가 초기 데이터 셋 구축이였다면 아주 훌륭하게 해낸 셈이지만, 자동화된 파이프라인 구동과 라이브 데모를 구축하는 것이 목표라면 실행이 될 때 마다 매우 느리고 비싸고 비효율적인 작업을 반복하는 것이 됩니다.

### 해결

오케스트레이션에서 실행되는 작업은 매일, 혹은 매 시간마다 반복되기 때문에 매번 모든 데이터를 처음부터 처리하는 것은 비용적으로도 시간적으로도 매우 비효율적입니다. 이 문제를 해결하는 가장 좋은 방법은 **증분 적재(Incremental Loading)**로, 쉽게 말해 마지막 실행 이후 변경된 데이터만 가져와 처리하는 방식입니다.

`updated_at` 필드를 이용하면 추출 시간과의 비교로 증분 추출 구현이 가능할 것입니다. 저장의 경우에도 S3가 기존의 데이터와 오늘 처리한 데이터들을 파티셔닝할 수 있도록 저장 구조를 변경해야 합니다. 이 경우에는 S3의 key 생성 규칙이 중요할 것입니다. 마지막으로 `dbt`의 변환 로직의 경우도 변경분만 병합할 수 있도록 `materialized` 설정을 이용해야 합니다.

### 결과

오케스트레이션 CI 파이프라인을 머지하기 전 해야 할 일입니다. 실제로 리팩토링 후에 어떤 결과가 나올지 확인해 보도록 하겠습니다.

## [Infra] S3 DTO 비용 줄이기

2025-11-11

> [!WARNING]
> S3 직접 링크의 문제점

### 문제

`dbt`에서 `read_json_auto`는 S3링크를 알아서 잘 읽는 것 같습니다. 와일드카드(`*`) 또한 자동으로 읽어 여러 개의 json을 한 번에 처리하기도 합니다. 매우 편리하지요.

하지만 이 방식의 문제점은 변환과 테스트 과정에서 S3에 반복하여 접속하면서 데이터 파일(`jsonl`)을 직접 다운받는다는 것입니다. 실제로 AWS는 데이터 외부 전송 비용을 매우 비싸게 잡고 있는데, 저는 이걸 간과하고 직접 DuckDB가 json 파일을 다운받게하여 `dbt run`과 `dbt test`를 진행하였습니다.

덕분에 GB당 $0.1에 달하는 비용을 70GB가 넘는 용량을 써버리고 말았고, 테스트 몇 회에 $10가 넘게 날아갔습니다. 왜 이렇게 많이 나왔을까요?

`dbt run` 단계에서 6회의 테이블 조인을 실행합니다. 이 과정에서 반복적으로 S3에서 game 데이터를 가져와야 하고, 결국 500MB 정도 되는 데이터 파일을 여러 번 읽으면서 몇 GB가 사용됩니다. 더 큰 문제는 테스트에 있는데 테스트 스위트는 총 41개이고, 현재는 아니지만 초기 테스트는 매 테스트마다 게임 테이블을 가져와야 했으므로, 얼추 계산해도 20GB 정도의 데이터 전송이 사용됩니다. 그럼 순식간에 4000원 정도가 날아갑니다.

### 해결

결국 S3에 직접 전송받는 방법을 포기해야합니다. 대신 CloudFront를 이용해 S3에 접근할 수 있도록 CloudFront 배포(Distribution)을 생성하고, 이 곳을 통해 파일을 읽어오는 방향으로 수정합니다. 이 방법은 AWS에서 권장하는 가장 표준적인 방법이기도 합니다. 이 방법이 효율적인 이유는 사용자는 CloudFront로 S3 데이터를 요청하고 CloudFront는 S3의 데이터를 가져와 전송해주기 때문인데, 여기서 CloudFront와 S3 사이의 전송 비용은 무료이기 때문입니다.

> 사용자 <-> CloudFront <-> S3
> 두 전송 과정에서 모두 비용 발생 X

참고로 CloudFront는 DTO의 프리 티어를 한 달에 1TB 제공합니다. 비용이 거의 안나오게 되는 것이죠.

다만 이렇게 되었을 때의 문제점은 `dbt`의 DuckDB가 `read_json_auto`가 CloudFront 링크를 S3만큼 똑똑하게 읽지 못한다는 점입니다. 예를 들어 와일드카드와 같은 문법을 이해하지 못하고, CloudFront URL을 http로 읽기 때문에 어떠한 처리도 하지 않습니다. 결국 과정마다 어떤 파일을 처리해야 할지 직접 지정해줘야 한다는 의미입니다.

따라서 저는 각 엔티티마다 `manifest.json`을 만들고, Loader에서 `manifest.json`에 처리해야 하는 대상을 업데이트해 준 뒤, `dbt`는 `manifest.json`을 읽고 어떤 데이터 파일을 처리 대상인지 알게하는 로직을 통해 서로의 역할을 분명히 하면서도 안전하게 접근할 수 있는 Best Practice를 구현하였습니다.

이는 특히 증분 작업에서 효과적인 방법입니다. 파이프라인은 매번 모든 데이터를 가져올 필요 없이 오늘(방금) 업데이트된 내용을 각 엔티티의 `manifest.json`을 확인하고, 변경된 데이터만 변환해 현재 데이터와 병합하면 되니까요.

### 결과

결국 `dbt`의 로직에 많은 변화가 있었습니다. CloudFront에서 대상 json 파일의 주소를 가져오는 로직은 `dbt`에서의 [매크로](../transform/macros/get_latest_partition.sql)를 사용해 함수화시켜 유연하게 적용이 가능했습니다. [`run_pipeline.py`](../scripts/run_pipeline.py)에도 `manifest.json` 관련 로직을 추가하면서 실질적인 오케스트레이션을 담당하고 있지만 점점 책임이 커지고 있는 문제를 해결하기 위해 적절한 함수화를 통해 역할 분리를 시켜놓은 상황입니다.

현재 오케스트레이션을 적용하기 위해 EL 파이프라인, `dbt`의 로직에 모두 변화가 생기다보니 곳곳에서 로직 상의 오류와 허점을 발견할 수 있는데요. 최적화와 기능 구현 사이에서 균형 잡기를 잘 해야겠다는 생각이 들었습니다.

## [Infra] 생명주기 관리

2025-11-12

> [!WARNING]
> S3에서는 데이터의 생명 주기를 어떻게 관리할까?

### 문제

S3를 실질적인 데이터 레이크로 쓰고 있는 상황에서, S3에 축적되는 데이터들을 어떻게 관리할 것인가에 대해 궁금해졌습니다.

실제로 데이터 파이프라인 과정에서 오류가 났을 때 불완전한 데이터 파일의 처리와도 맞닿아 있는 문제이기도 한데요. 위에서 언급한 `manifest.json`과 증분 적재를 위한 상태 값 저장소인 `S3`의 버킷 내 State 저장소는 엔티티 각각의 수집 작업이 완료되어야 쓰기 작업을 진행하기 때문에 원자성을 보장합니다. 다만 실제 데이터들의 경우, 배치 후 바로 적재되기 때문에 불일치가 일어납니다. 이는 작업 과정에 문제를 일으키지 않지만 필요 없어진 데이터들이 S3에 계속해서 축적될 수 있다는 리스크를 가집니다.

### 해결

이를 유연하게 해결하는 방법은 S3내의 수명 주기 규칙(Lifecycle Rule)를 생성하는 것입니다. 그 중에서도 객체 태그(Tag)를 이용한 규칙의 경우 쉽게 구성해서 적용해 볼 수 있었는데요.

버킷(Bucket)의 관리 탭으로 가면 수명 주기를 만들 수 있습니다. 여기서 특정 객체의 키-값쌍으로 이루어진 태그와 일치하면 객체를 만료시킬 수 있습니다. 저는 구체적으로 S3에 적재할 때 `status: temp`로 태그를 달아놓고, `manifest.json`과 State를 업데이트할 때 같이 객체들의 태그를 변경해 최종 저장하여 최대한의 원자성을 지키고자 했습니다. 만약 `status`태그에 `temp` 값을 유지하는 객체들이 있다면 적재 과정에서 비정상적으로 종료된 것이고, 하루 뒤 만료되어 사라집니다.

### 결과

S3는 기본 용량이 큰 만큼 토이 프로젝트 수준에서 민감하게 객체 주기를 관리할 필요는 없지만, 사용한 수명 주기 규칙의 경우 확장성을 고려한 대규모 데이터 레이크 관리의 정석적인 패턴이라고 할 수 있습니다.

만약 태그를 여러개 붙이고 싶다면 AND 조건과 같은 방법을 활용해 태그 필터의 조건을 강화할 수도 있습니다. 하지만 태그는 최대한 단순하게 유지하는 것이 Best Practice라고 합니다. 복잡한 시나리오는 실수를 유발할 수 있으니까요.

태깅을 위해 `loader`에서 S3에 적재할 때 기본 태그를 달고, `run_pipeline.py`에서 작업 완료 후에 완료 태그로 변경하는 로직을 구현함으로서 해당 부분은 개선되었습니다.
