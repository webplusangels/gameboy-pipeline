# 02. 기술 스택 및 이유

이 프로젝트는 작지만 현대적이고 견고한 파이프라인 구축을 최우선으로 생각하고 있습니다. 문서에서 후술할 기술 스택들은 이 목표를 중심으로 선정되었으며 그 외의 선정 이유 또한 본 문서에서 상세하게 설명합니다.

## [`uv`](https://github.com/astral-sh/uv) - **가상환경 및 패키지 관리**

### 선택한 이유

Python 가상환경의 경우 선택지가 아주 다양하거나 편리하지는 않은 것으로 알고 있습니다. 기존에는 `venv`, `poetry` 등을 써왔지만 이번 프로젝트의 경우, 파이프라인의 CI/CD 속도와 로컬 개발 생산성을 극대화하기 위해 `uv`를 채택하였습니다.

가장 큰 장점은 익숙한 가상 환경인 `poetry`와 `pip`같은 패키지 관리자 대비 [수십 배 빠른 의존성 설치 속도](https://github.com/astral-sh/uv/blob/main/BENCHMARKS.md)입니다. 자동화된 CI 파이프라인이 실행될 때마다 즉각적인 테스트 결과를 얻을 수 있어 TDD 워크플로우의 효율을 극대화합니다. 추가로 `uv sync`와 `uv run` 등의 명령어는 `poetry`와도 비슷하여 사용하기에 편리했습니다.

### 대안

**`Poetry`**

기존에 가장 많이 사용해 익숙한 가상환경 관리자입니다. 사용자 규모나 안정성 측면에서 `uv`보다 확실하게 앞서지만, 이번에는 _'작고 빠른'_, _'현대적'_ 이라는 목표로 빠른 속도의 CI 파이프라인을 구축하고자 선택하지 않았습니다.

`uv`의 경우 현재 기준(2025.11) 1.0 릴리즈가 되지 않은 상태입니다. 그만큼 리스크가 없지는 않지만, Ruff 툴체인의 일부로 Astral에서 개발중이라는 점, 속도 면에서 압도적으로 빠르다는 점을 고려해 Python에서의 개발 편의성을 극대화할 수 있을 것이라 생각합니다.

## Github Actions - **지속적 통합(CI) 파이프라인**

### 선택한 이유

Github Actions는 가벼운 경험도 있고, 프로젝트 관리를 Github를 통해 하고 있는 상황에서 쓰지 않을 이유가 없습니다. Github와의 원활한 통합을 포함해서, 이를 대시보드로 보여주고, 거의 무료인데다 워크플로우 정의가 직관적이고 할 수 있는 작업이 의외로 다양한 점 등을 장점으로 생각합니다.

특히 TDD를 채택한 이상, 테스트 코드와의 통합이 원활하고 환경 변수 등의 관리가 Github와 연동된다는 점도 편리한 점입니다. 추가적으로 현재 ci 파이프라인에서는 린팅과 테스트를 병렬 처리하고, 유닛 테스트와 통합 테스트를 직렬로 처리하는 등의 현 시점에서 필요한 플로우의 정의도 쉬우며 `uv`가 pip 패키지를 가져올 때 캐싱 처리도 되어 시간을 더욱 단축시킬 수 있다는 점도 좋은 것 같습니다.

### 대안

**`Jenkins`**

비교적 작은 개인 프로젝트인 만큼, 다른 CI/CD 도구와 비교할 여지가 크게 없었지만 대표적으로 쓰이는 `Jenkins`가 있습니다. 하지만 운영 시 복잡도는 비교가 불가능할 만큼 높으며, 유지 비용 또한 적지 않게 생기게 됩니다. 사실상 업계 표준이며, 유연하고 확장성이 높다는 장점이 있지만 그 부분이 필요한 애플리케이션이 아닌 만큼(토이 프로젝트라면 대부분 그렇다고 생각합니다) 현재는 도입할 이유를 찾는 것이 더 어려운 것 같습니다.

**`GitLab Ci/CD`, `CircleCI`**

GitLab을 쓴다면 고려해 볼 만한 옵션입니다. Github Actions보다 더 많은 기능들을 제공한다고 알려져있고, 특히 엔터프라이즈급 기능에서 차이가 많이 나는 것으로 알고 있습니다. 다만 제 프로젝트와는 비교 우위에서 겹치는 것이 없으므로 후보군에서 제외합니다.

`CircleCI`는 흔히 `Docker`와 `K8s`와의 통합에서 강력한 이점을 가진다고 알고 있습니다. 유료 플랜이 존재하지만 무료 플랜의 수준에서 충분한 경우가 많고 언급했다시피 컨테이너 환경에서는 더 적합한 툴이지만 현재 시점인 애플리케이션 완성 이전에 컨테이너화 계획은 없으며, 앞으로도 Github Actions의 빌드 과정에서 심각한 병목이 발생하지 않는 이상, 바꿔야 할 필요는 없습니다.

## `pytest` - **Python 테스팅 도구**

### 선택한 이유

`pytest`는 Python 내장 라이브러리는 아니지만, 테스트 옵션 중에서 사실상 표준처럼 여겨지는 라이브러리입니다.

문법이 간결하며, 함수 구조이기 때문에 테스트 코드의 이해가 쉽고 간단하며 명확하게 작성됩니다. 이는 현재 개발 방법론으로서의 목표로 삼고 있는 `TDD`에도 적합합니다. Fixture, 테스트 설정 등의 방법도 비교적 간단하며, 테스트 실패 시에도 출력이 매우 상세하게 출력되고, 여러 플러그인을 통해 mock, coverage 등 테스팅에 필요한 기능들을 내장 기능처럼 제공합니다.

### 대안

**`unittest`**

Python에서 유닛 테스트의 내장 라이브러리로, 설치가 필요없습니다. 다만 객체 지향 스타일로 접근하기 때문에 작성 방법에서 `pytest`와 차이가 있으며 직관적이지는 않다고 개인적으로 느꼈습니다. 더해서 앞에서 언급했던 `pytest`가 보유한 풍부한 확장 기능들과의 비교했을 때도 편의성이 좋다고 하기에 무리가 있으며, 실패 시 디버깅과 같은 많은 테스트를 실행하고 작성해야 하는 환경 등에서는 적합하지 않다고 생각합니다.

## `httpx` - **비동기 요청 라이브러리**

### 선택한 이유

`httpx`를 선택한 가장 큰 이유는 비동기를 네이티브로 지원하는 클라이언트이기 때문입니다.

데이터 파이프라인은 일반적으로 많은 양의 데이터를 상정하는 만큼, 요청을 비동기 방식으로 날려 I/O 대기 시간을 최소화해야 할 필요성이 생깁니다. 이전에 구현했던 파이프라인도 동기 방식을 사용한 한 Extract 과정이 요청 제한과 맞물려 병목이 생기고 태스크 중 가장 큰 시간을 소모한 만큼, 비동기 요청의 필요성은 체감하고 있었습니다.

또한 `httpx`는 `aiohttp`와 다르게 `requests`와 거의 동일한 구조를 가지고 있어 직관적입니다. 또한 Http/2 지원, 타임아웃 설정이나 연결 풀링 등 프로덕션 환경에 필요한 기능들을 전부 제공하기도 합니다. 비동기 처리를 극대화하기 위한 더 자세한 기능들은 문서를 통해 확인하고 활용할 예정입니다.

추가로 타입 힌트도 잘 구현되어 있어 `mypy`와 함께 사용할 때 타입 안정성도 보장됩니다.

### 대안

**`requests`**

Python 생태계에서 사실상 표준으로 사용되는 HTTP 라이브러리입니다. 안정적이고 성숙했으며, 방대한 커뮤니티와 레퍼런스를 가지고 있습니다. 간단한 스크립트나 동기 처리만 필요한 경우라면 최고의 선택이지만, 역시 비동기 처리가 필요한 제 파이프라인에서는 부적합합니다.

비동기를 위해 `aiohttp`와 함께 사용하는 방법도 있지만, 이는 두 라이브러리의 차이를 비교 학습해야 하고 코드베이스가 복잡해지는 단점이 있습니다.

**`aiohttp`**

순수 비동기 HTTP 클라이언트로, `httpx`보다 먼저 나온 라이브러리입니다. 성능 면에서는 [`httpx`와 비슷하거나 더 빠를 수 있지만 (대부분 더 빠르다고 합니다.)](http://stackoverflow.com/questions/78516655/why-is-httpx-so-much-worse-than-aiohttp-when-facing-high-concurrent-requests), API의 사용 방법이 `requests`와 조금 다르고 세션 관리가 좀 더 복잡하다고 합니다. 현재로서는 도입 예정에 없지만, 속도를 최적화하기 위해 실험해 볼 여지는 있습니다.

## `dbt` - **데이터 빌드 툴**

### 선택한 이유

`dbt`는 ELT 파이프라인의 T를 담당합니다. 정확하게는 T를 계획하는, 데이터 간의 의존 관계를 파악하고, 쿼리문을 적절한 순서로 실행할 수 있도록 정의하는 존재입니다

하지만 단순하게 데이터를 변환하는 것이 아니라 신뢰할 수 있고, 유지 보수가 가능하며, 파일 형태 그 자체로 문서화되어 데이터를 관리하도록 강제하는 프레임워크라고 할 수 있습니다. 이러한 `dbt`의 특성은 파일 형태로 스크립트를 이용해 데이터 변환에 집중할 수 있게 하는 것인데요. 이렇게 ELT 각 단계에 최적화된 기술을 쓴다는 것은 파이프라인 내에서 서로 다른 기술로 단계를 명확하게 정의하고, 서로 결합도를 낮추도록 관심사의 분리를 강제할 수 있는 방법이기도 합니다.

제가 만들고자 하는 파이프라인에 필요한 기술의 본질과 그 판단 기준은 결국 결과물에 걸맞지 않은 과도한 기술인지, 그리고 현대적인지 정도로 요약할 수 있는데요. `dbt`는 SQL(jinja)로 변환 로직을 작성하도록 할 수 있으며, 데이터 간 의존성을 파악하는 리니지 그래프를 제공합니다. 거기에 스크립트의 테스트가 가능하고, `Snowflake`, `BigQuery`, `DuckDB` 등 필요에 맞는 기술들을 네이티브에 가깝게 지원하기 때문에 구현하고자 하는 기술에 한해 이보다 더 알맞은 기술은 없다고 생각합니다.

### 대안

**Python/`Pandas`/`PySpark` 스크립트**

제가 이전에 사용했던 Python 스크립트로 제작한 파이프라인에서는 ELT가 아닌 ETL 순서로 데이터를 변환하였고, 모든 과정이 모듈 단위로만 쪼개져 있어 결합도가 높아 수정이 사실상 위험하고, 테스트 코드를 작성하는 것 자체가 곤란했습니다. 스크립트 단위라면 세밀하게 설계하지 않는 이상 관심사의 분리가 쉽지 않겠죠.

경험한 바에 따르면 결국 Python 스크립트의 경우 프로토타이핑에 능하다고 할 수 있습니다. `Pandas` 또한 사실상 Python의 표준 기술이기도 하구요. 하지만 코드 자체로는 스크립트가 길고 복잡하며, 데이터가 커지면 커질수록 비효율적인 작업이 될 가능성이 높습니다. 또한 Python은 SQL이 아니기 때문에, 데이터 엔지니어가 아닌 분석가가 SQL에만 능하다면 (Python보다는 쉬우니까) 설계자가 아니라면 쉽사리 수정은 불가능하겠죠.

또한 Python으로 작성하는 `PySpark` 코드의 경우엔 경량 파이프라인에는 걸맞지 않으며, 제가 다루는 데이터 규모에도 맞지 않는다고 볼 수 있습니다.

## `Github Actions` vs. `Airflow` or `Dagster` - **오케스트레이션**

## 선택한 이유

파이프라인을 구상할 때 엔터프라이즈 환경에서 주로 쓰이는 `Airflow`를 가장 먼저 떠올린 것은 사실입니다. 다만 `Airflow`는 실무 환경에서 널리 쓰이는 소프트웨어답게 오픈 소스라는 장점을 제외하면 간단히 선택할 수 있는 옵션은 아니였습니다.

이유로는 대표적으로 **1. 비용 문제 (24시간 서버)**, **2. 인프라 설정의 복잡성**을 꼽을 수 있습니다. 때문에 다음과 같이 두 가지의 목표를 잡고 로드맵을 구성합니다.

### 서버리스 스택

이 스택은 서버리스를 이용해 한국 시간 새벽 4시에 매일 파이프라인이 자동 실행되는 시스템으로, 우선적으로 완성되어 **라이브 데모**로 사용될 예정입니다.

공통적으로 사용하는 EL 로직을 제외하고, T의 경우 현재 사용하는 `dbt-duckdb`를 사용해 S3에서 원시 데이터를 `DuckDB`에서 `dbt` 모델을 실행해 처리하고, 이 데이터를 S3에 `Parquet` 형태로 저장하게 됩니다.

Orchestration의 경우 `Github Actions`의 `cron`으로 자동 실행되는 방식으로 진행됩니다. 장점으로는 서버리스로 24시간 띄우지 않아도 되며, 비용 부담에서 완전히 자유롭습니다. 또한 `DuckDB`등의 열 기반 경량 데이터베이스를 사용함으로서 데이터 규모에 비해 과도한 기술 대신 합리적인 선택을 했다고 판단할 수 있었습니다.

대신 데이터를 실시간으로 처리할 수 없다는 점이 치명적이지만, 이는 `Streamlit`등의 무료 호스팅 서비스를 이용해 대시보드로 사용자가 직접 데이터의 신뢰성을 확인할 수 있고, 동적 필터링을 통한 단순 덤프 이상의 여러가지 비즈니스 가치를 제공할 수 있는 라이브 데모로 활용하여 상쇄할 예정입니다.

### 엔터프라이즈 스택

이 스택은 실제 엔터프라이즈 환경에서 자주 쓰이는 기술들로 이루어진 시스템으로, 단지 사용했다는 것에 그치지 않고, **재현성**을 중점으로 최대한 구현될 예정입니다.

공통 EL 로직을 제외하고, T의 경우 `AWS Glue` 혹은 `AWS Athena`와 같은 대규모 분산 처리 엔진 위에서 `dbt`를 실행할 수 있도록 합니다. 데이터 규모에 비해 합리적이지는 않지만, 실무에서 자주 쓰이는 기술들에 대한 경험, 그리고 서비스를 구축할 때의 비용 최적화 경험과 왜 대규모 데이터 환경에서는 이 기술을 쓰는지에 대한 실제 인사이트를 얻을 수 있는 좋은 기회라고 생각합니다.

Orchestration 같은 경우, 로컬에서도 완벽하게 제어할 수 있도록 `Airflow`와 `Docker`의 조합으로 실제 복잡한 의존성 관리나 재시도 정책, `Airflow`에서만 가능한 백필(Backfill)등의 기능의 설정에 대해 체득하고 실제로 다룰 수 있는 능력을 키웁니다. 역시 설정 면에서 비효율적이고 구현까지 못 갈 수도 있지만 실제 구현한 데이터 파이프라인을 새로운 기술 스택에서 가동시키는 것(혹은 시도)에 대한 경험은 앞서 말했듯 충분한 가치가 있을 것으로 예상합니다.

이렇게 만들어진 결과물은 배포를 유지시키는 것은 비용적으로 비효율적이라 판단하고, 이 경험을 어떻게 녹여낼 것인가에 대해 고민하고 있습니다. 예를 들어 현재는 실제 구축한 아키텍처 다이어그램과 스크린샷, `dbt` 리니지 그래프, `Athena` 쿼리와 같은 모습을 `README.md`에 첨부하고, 재현성을 위해 `docker-compose`로 컨테이너화하여 테스트할 수 있도록 가이드를 추가를 고민 중입니다.

### 결론

요약하자면, 첫 번째로 시간-비용 효율적인 오케스트레이션을 위해 `GitHub Actions` 워크플로우를 구축하고, EL 작업이 완료되면 T 작업이 S3를 타겟으로 실행되도록 파이프라인을 자동화합니다. 자동화한 파이프라인은 `Streamlit` 라이브 데모를 통해 확인할 수 있습니다.

두번째 시도로 `GitHub Actions`에서 부족한 지점을 알아보고, 왜 전문 오케스트레이터가 필요한지 깨닫기 위해 `Airflow`/`Dagster`로 마이그레이션하여, 엔터프라이즈급 스케줄링과 모니터링을 구현해보고 단기간 운영해 본 후 차이점을 비교 정리해봅니다.

## `DuckDB` - **경량 분석 데이터베이스**

### 선택한 이유

`DuckDB`는 OLAP(Online Analytical Processing, 온라인 분석 처리) 워크로드에 최적화된 임베디드 분석 데이터베이스입니다. 이 프로젝트에서 Transform 계층의 실행 엔진으로 선택한 가장 큰 이유는 별도의 서버 없이 `dbt`를 실행할 수 있을 뿐만 아니라, 네이티브에 가깝게 통합되어 있기 때문입니다.

전통적인 데이터베이스 시스템(`PostgreSQL`, `MySQL` 등)의 경우 24시간 가동되는 서버가 필요하고, 클라우드 데이터 웨어하우스(`Snowflake`, `BigQuery`)는 쿼리당 비용이 발생합니다. 두 방법 모두 만만찮은 비용이 발생한다고 볼 수 있습니다. 반면 `DuckDB`는 프로세스 내에서 실행되기 때문에 인프라 비용이 0원이며, GitHub Actions 같은 서버리스 환경에서도 즉시 사용할 수 있습니다.

추가로 컬럼 기반 스토리지 아키텍처 덕분에 분석 쿼리에 매우 빠르고, Parquet/JSON 같은 파일 포맷을 네이티브로 읽을 수 있어 S3에 저장된 데이터를 별도의 적재 과정 없이 바로 쿼리할 수 있습니다. 특히 `read_json_auto()` 함수는 스키마를 자동으로 추론해주어 개발 속도를 크게 향상시킵니다.

```sql
-- S3의 JSONL을 바로 읽어 쿼리
SELECT * FROM read_json_auto('s3://bucket/raw/games/*.jsonl')
WHERE updated_at > 1732435200;
```

다만 DTO 비용 때문에 CloudFront로 옮겨 사용해야 했습니다. 이때는 `read_json_auto()` 함수가 CloudFront url을 http로 인식하기 때문에 와일드카드와 같은 문법이 달라져 조금 더 수정이 필요했습니다.

앞서 말했듯 `dbt-duckdb` 어댑터가 존재하기 때문에 둘의 통합에도 문제가 없습니다. `profiles.yml`에서 S3 credential만 설정하면 dbt 모델이 CloudFront를 통해 S3 데이터를 읽고, 변환 결과를 다시 S3 Parquet로 저장하는 전체 플로우가 SQL만으로 완성됩니다.

추가로 타입 시스템이 PostgreSQL과 호환되며 표준 SQL을 지원하기 때문에 규모가 커질 경우 `Snowflake`나 `BigQuery`로 마이그레이션할 때도 쿼리 수정이 최소화됩니다.

### 대안

**`PostgreSQL`**

가장 대중적인 관계형 데이터베이스로, 안정성과 확장성이 검증되었습니다. 이전 파이프라인에서 쓰였던 데이터베이스이기도 합니다. 하지만 대부분의 행기반 관계형 DBMS가 그렇듯 OLTP(트랜잭션 처리)에 최적화되어 있어 분석 쿼리에서는 `DuckDB`보다 느리고, 무엇보다 비용이 발생하는 바람에 소규모 분석 파이프라인에서는 과도한 인프라이며, 서버리스 환경에서는 사용할 수 없다는 판단을 하였습니다.

**`SQLite`**

`DuckDB`처럼 임베디드 DB이지만 역시 행기반으로 OLTP 워크로드에 최적화되어 있어 대용량 집계 쿼리에서는 성능이 떨어집니다. 특히 S3 파일을 직접 읽는 기능이 없어 적재 단계에 추가적인 구현이 필요합니다.

분석 워크로드의 경우 `DuckDB`가 압도적으로 유리합니다.

**`AWS Athena`**

S3를 직접 쿼리할 수 있는 서버리스 쿼리 엔진으로, 프로덕션 환경에서는 할법한 선택지입니다. 하지만 쿼리당 스캔한 데이터 용량만큼 비용이 청구되며($5/TB), 개발 단계 특성상 반복 테스트 시 비용이 초과될 수 있습니다.

장기 로드맵에서 엔터프라이즈 스택으로 실험해볼 계획이지만, 현재 초기 구현에서는 비용 효율적인 `DuckDB`가 더 적합합니다.

## `S3` - **클라우드 객체 스토리지**

### 선택한 이유

`S3`는 AWS의 객체 스토리지 서비스로, 이 프로젝트에서는 데이터 레이크(Data Lake)의 역할을 합니다. Extract 단계에서 수집한 원본 데이터(JSONL)와 Transform 단계의 최종 결과물(Parquet)을 모두 S3에 저장합니다.

선택한 핵심 이유는 다음과 같습니다:

**1. 사실상 표준**

데이터 엔지니어링 생태계에서 S3는 업계 표준이라고 합니다. `dbt`, `Spark`, `Athena`, `DuckDB` 등 거의 모든 데이터 도구가 S3를 우선적으로 지원하며, S3 호환 스토리지(`MinIO`, `R2` 등)도 많아 벤더 락-인 위험이 적습니다.

**2. 비용 효율성**

스토리지 저장 비용이 매우 저렴합니다. 현재 프로젝트 규모의 경우 월 $0.03 미만이며, 데이터 관리에도 티어 이동, 태그를 이용한 정책 설정 등 유용한 기능들이 많습니다.

**3. 내구성과 가용성**

99.999999999%의 내구성을 보장하며, 리전 내 여러 AZ에 자동 복제됩니다. 백업이나 복제 전략을 별도로 구축할 필요가 없습니다.

**4. 데이터 레이크 아키텍처 구현**

S3의 키(Key) 구조로 파티셔닝을 구현할 수 있습니다:

```
s3://bucket/
├── raw/
│   ├── dimensions/
│   │   ├── platforms/batch-001.jsonl
│   │   └── genres/batch-002.jsonl
│   └── games/
│       ├── dt=2025-01-01/batch-001.jsonl
│       └── dt=2025-01-02/batch-002.jsonl
└── marts/
    └── dim_games/dim_games.parquet
```

이러한 구조는 `Hive 파티셔닝` 관례를 따르기 때문에 `Athena`, `Spark`, `Presto` 같은 도구에서도 바로 인식됩니다.

**5. CloudFront와의 통합**

S3 앞단에 CloudFront CDN을 배치하면 대부분의 데이터 전송 비용(DTO)을 절감할 수 있습니다. dbt가 S3에서 직접 읽으면 DTO가 $0.09/GB인 반면, CloudFront를 경유하면 $0.01/GB로 줄어듭니다. 다만 CloudFront를 경유하는 경우에는 `dbt`나 `DuckDB`의 네이티브한 지원을 어느정도 우회하는 방식이라 추가 로직이 필요합니다.

```sql
-- dbt 모델에서 CloudFront를 통해 읽음
SELECT * FROM read_parquet(
  'https://d1234abcd.cloudfront.net/marts/dim_games/dim_games.parquet'
)
```

### 대안

**로컬 파일 시스템**

개발 환경 기준 로컬 파일 시스템을 사용하지만, 프로덕션에서는 당연하게도 내구성, 확장성, 협업 측면에서 부족합니다. GitHub Actions 같은 일시적 환경에서는 여러 방향에서 접근할 수 있는 스토리지가 필요합니다.

**`Google Cloud Storage (GCS)`**

S3와 유사한 객체 스토리지로, BigQuery와의 통합이 우수합니다. 하지만 이 프로젝트는 AWS 생태계(CloudFront, Athena 등)와 통합하기 때문에 S3가 더 적합합니다. 비용적으로는 거의 동일합니다.

**`Azure Blob Storage`**

마이크로소프트 생태계에서 사용한다면 좋은 선택이지만, Python/dbt 생태계에서는 S3가 우세하고, 위와 같은 이유로 선호되지 않습니다.

**`MinIO` (셀프 호스팅)**

S3 호환 오픈소스 스토리지로, 홈 서버를 직접 구축하는 온프레미스로 비용 절감을 고려할 수 있습니다. 하지만 서버 자원이나 운영 부담이 있고, 소규모 프로젝트에서는 S3의 관리형 서비스가 더 효율적입니다.
